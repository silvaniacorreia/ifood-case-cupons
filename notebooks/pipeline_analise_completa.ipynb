{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe50670",
   "metadata": {},
   "source": [
    "# Análise Completa - Case Ifood: Teste A/B Estratégia de Cupons\n",
    "\n",
    "Notebook único para orquestrar as tarefas de execução de *setup*, **ETL** e análise dos dados, integrando os diferentes módulos do repositório de origem:\n",
    "\n",
    "- Clona/atualiza o repositório do projeto, com as dependências, no Colab\n",
    "- Instala dependências e faz o **download** dos dados brutos\n",
    "- Sobe Spark e executa o **ETL** (orders/consumers/restaurants + mapa A/B)\n",
    "- Mantém `orders_silver` e `users_silver` em memória\n",
    "- Realiza a análise exploratória dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46b6e4",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc5c4c",
   "metadata": {},
   "source": [
    "### Configuração de Ambiente e Download de Dados Brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daddc1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "GITHUB_USER = \"silvaniacorreia\"\n",
    "REPO_NAME   = \"ifood-case-cupons\"\n",
    "REPO_URL    = f\"https://github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "\n",
    "def run(cmd):\n",
    "    print(\">\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# 1) clonar/atualizar repositório\n",
    "ROOT = Path(\"/content\")\n",
    "PROJECT_DIR = ROOT / REPO_NAME\n",
    "if not PROJECT_DIR.exists():\n",
    "    run([\"git\", \"clone\", REPO_URL, str(PROJECT_DIR)])\n",
    "else:\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    run([\"git\", \"fetch\", \"--all\"])\n",
    "    run([\"git\", \"reset\", \"--hard\", \"origin/main\"])\n",
    "    run([\"git\", \"checkout\", \"main\"])\n",
    "    run([\"git\", \"pull\", \"origin\", \"main\"])\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# 2) deps + download programático\n",
    "run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\", \"--no-cache-dir\"])\n",
    "run([sys.executable, \"scripts/download_data.py\"])\n",
    "\n",
    "# 3) sys.path\n",
    "if str(PROJECT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_DIR))\n",
    "print(\"✔️ Bootstrap concluído. Projeto:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f4bcb",
   "metadata": {},
   "source": [
    "### Iniciando o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66ab3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import load_settings, get_spark\n",
    "\n",
    "s = load_settings()  \n",
    "extra = dict(getattr(s.runtime.spark, \"conf\", {}) or {})\n",
    "extra.setdefault(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "spark = get_spark(\n",
    "    app_name=s.runtime.spark.app_name,\n",
    "    shuffle_partitions=s.runtime.spark.shuffle_partitions,\n",
    "    extra_conf=extra,\n",
    ")\n",
    "print(\"✔️ Spark ativo - versão:\", spark.version)\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5aaf2e",
   "metadata": {},
   "source": [
    "### Análises Pré-Flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc9f02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Checagens dados brutos\n",
    "from src.checks import preflight\n",
    "from pprint import pprint\n",
    "\n",
    "rep = preflight(s.data.raw_dir, strict=False)\n",
    "print(\"Pré-flight (resumo):\")\n",
    "pprint({\n",
    "    \"raw_dir\": rep[\"raw_dir\"],\n",
    "    \"orders_format_guess\": rep[\"orders_format_guess\"],\n",
    "    \"files\": {k: {kk: vv for kk, vv in v.items() if kk in (\"exists\",\"size_bytes\",\"gzip_ok\",\"tar_ok\")} for k, v in rep[\"files\"].items()},\n",
    "    \"ab_csv_candidates\": rep[\"ab_csv_candidates\"][:3],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc2464",
   "metadata": {},
   "source": [
    "### ETL (Extração, Transformação e Carga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1918c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src import etl, checks\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "def _get_exp_window(s):\n",
    "    \"\"\"\n",
    "    Lê a janela do experimento a partir das configurações. Caso não exista, utiliza inferência automática.\n",
    "\n",
    "    Parâmetros:\n",
    "        s: Objeto de configurações carregado.\n",
    "\n",
    "    Retorna:\n",
    "        Tuple[str, str, bool]: Data de início, data de fim e flag de inferência automática.\n",
    "    \"\"\"\n",
    "    win = getattr(s.analysis, \"experiment_window\", None)\n",
    "    if isinstance(win, dict):\n",
    "        start = win.get(\"start\")\n",
    "        end   = win.get(\"end\")\n",
    "    else:\n",
    "        start = None\n",
    "        end   = None\n",
    "    auto = bool(getattr(s.analysis, \"auto_infer_window\", True))\n",
    "    return start, end, auto\n",
    "\n",
    "start, end, auto = _get_exp_window(s)\n",
    "\n",
    "# 1) Leitura dos dados brutos\n",
    "orders, consumers, restaurants, abmap = etl.load_raw(spark, s.data.raw_dir)\n",
    "checks.profile_loaded(orders, consumers, restaurants, abmap, n=5)\n",
    "\n",
    "# 2) Limpeza e conformidade dos dados\n",
    "# Inclui normalização de timezone e aplicação de janela experimental\n",
    "# Utiliza quantis para robustez contra outliers\n",
    "df = etl.clean_and_conform(\n",
    "    orders, consumers, restaurants, abmap,\n",
    "    business_tz=getattr(s.analysis, \"business_tz\", \"America/Sao_Paulo\"),\n",
    "    treat_is_target_null_as_control=getattr(s.analysis, \"treat_is_target_null_as_control\", False),\n",
    "    experiment_start=start,\n",
    "    experiment_end=end,\n",
    "    auto_infer_window=auto,\n",
    "    use_quantile_window=True,     \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3) Ajustes finais e agregações para análise\n",
    "orders_silver = etl.build_orders_silver(df)\n",
    "orders_silver = etl.enrich_orders_for_analysis(orders_silver)\n",
    "users_silver  = etl.build_user_aggregates(orders_silver, start, end)\n",
    "\n",
    "# 4) Cálculo de recência com base no último timestamp observado\n",
    "ref_ts = orders_silver.agg(F.max(\"event_ts_utc\")).first()[0]\n",
    "users_silver = users_silver.withColumn(\"recency\", F.datediff(F.lit(ref_ts), F.col(\"last_order\")))\n",
    "\n",
    "# 5) Salvar resultados em formato Parquet (opcional)\n",
    "SAVE_PARQUET = False\n",
    "if SAVE_PARQUET:\n",
    "    (\n",
    "        orders_silver\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"event_date_brt\")\n",
    "        .parquet(f\"{s.data.processed_dir}/orders_silver.parquet\")\n",
    "    )\n",
    "    users_silver.write.mode(\"overwrite\").parquet(f\"{s.data.processed_dir}/users_silver.parquet\")\n",
    "\n",
    "# 6) Contagem de linhas para validação\n",
    "print(\"orders_silver:\", orders_silver.count(), \"linhas\")\n",
    "print(\"users_silver :\", users_silver.count(), \"linhas\")\n",
    "\n",
    "# 7) Exibição de amostras para validação\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(orders_silver.limit(5).toPandas())\n",
    "    display(users_silver.limit(5).toPandas())\n",
    "except Exception as e:\n",
    "    print(\"Aviso: toPandas falhou, mostrando via Spark .show()\")\n",
    "    orders_silver.show(5, truncate=False)\n",
    "    users_silver.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8d32d",
   "metadata": {},
   "source": [
    "### Checagem dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78d984",
   "metadata": {},
   "source": [
    "Foram investigadas duplicatas semânticas na fato (IDs diferentes com mesmo cliente/loja/tempo/valor). Como apenas 1 caso foi encontrado, o que gera efeito desprezível, não foi aplicada a deduplicação adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4388b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def check_post_etl(\n",
    "    orders_silver,\n",
    "    users_silver,\n",
    "    *,\n",
    "    light: bool = True,\n",
    "    key_cols: list[str] | None = None,\n",
    "    sample_frac: float = 0.001,\n",
    "    preview_rows: int = 5,\n",
    "    use_pandas_preview: bool = False,\n",
    "    check_semantic_dups: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executa checagens leves pós-ETL para registro no Colab.\n",
    "    - light=True: nulos apenas em colunas-chave e previews por sample.\n",
    "    - light=False: nulos em todas as colunas (lento).\n",
    "    - check_semantic_dups: investiga duplicatas semânticas na fato (lento moderado).\n",
    "    \"\"\"\n",
    "    if key_cols is None:\n",
    "        key_cols = [\n",
    "            \"order_id\", \"customer_id\", \"merchant_id\",\n",
    "            \"event_ts_utc\", \"order_total_amount\",\n",
    "            \"is_target\", \"price_range\", \"language\", \"active\",\n",
    "            \"delivery_time_imputed\", \"minimum_order_value_imputed\",\n",
    "        ]\n",
    "    key_cols = [c for c in key_cols if c in orders_silver.columns]\n",
    "\n",
    "    print(\"Faixa de datas (UTC) em orders_silver:\")\n",
    "    orders_silver.agg(\n",
    "        F.min(\"event_ts_utc\").alias(\"min_utc\"),\n",
    "        F.max(\"event_ts_utc\").alias(\"max_utc\"),\n",
    "    ).show(truncate=False)\n",
    "\n",
    "    print(\"Split A/B (users):\")\n",
    "    users_silver.groupBy(\"is_target\").count().orderBy(\"is_target\").show()\n",
    "\n",
    "    # --- Nulos em orders_silver ---\n",
    "    def nulls_by_col(df, cols):\n",
    "        exprs = [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in cols]\n",
    "        return df.select(exprs)\n",
    "\n",
    "    if light:\n",
    "        print(f\"Nulos (colunas-chave): {key_cols}\")\n",
    "        nulls_by_col(orders_silver, key_cols).show(truncate=False)\n",
    "    else:\n",
    "        print(\"Nulos (todas as colunas) — operação pesada:\")\n",
    "        nulls_by_col(orders_silver, orders_silver.columns).show(truncate=False)\n",
    "\n",
    "    # --- Duplicatas semânticas (order_ids diferentes com mesmo cliente/restaurante/ts/valor) ---\n",
    "    if check_semantic_dups:\n",
    "        print(\"\\nPossíveis duplicatas sistêmicas (mesmo cliente/restaurante/ts/valor, order_id distinto):\")\n",
    "        dups = (\n",
    "            orders_silver\n",
    "            .groupBy(\"customer_id\", \"merchant_id\", \"event_ts_utc\", \"order_total_amount\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"order_id\").alias(\"n_orders\"),\n",
    "                F.collect_set(\"order_id\").alias(\"order_ids\"),\n",
    "            )\n",
    "            .filter(F.col(\"n_orders\") > 1)\n",
    "        )\n",
    "        total_dups = dups.count()\n",
    "        print(f\"Total de combinações com múltiplos order_id: {total_dups}\")\n",
    "        if total_dups > 0:\n",
    "            dups.select(\"customer_id\",\"merchant_id\",\"event_ts_utc\",\"order_total_amount\",\"n_orders\",\"order_ids\")\\\n",
    "                .orderBy(F.col(\"n_orders\").desc())\\\n",
    "                .show(10, truncate=False)\n",
    "\n",
    "    # --- Previews rápidos ---\n",
    "    print(\"\\nPreview orders_silver (sample leve):\")\n",
    "    orders_preview_cols = [c for c in [\n",
    "        \"price_range\",\"order_id\",\"customer_id\",\"merchant_id\",\n",
    "        \"event_ts_utc\",\"order_total_amount\",\"origin_platform\",\n",
    "        \"is_target\",\"language\",\"active\"\n",
    "    ] if c in orders_silver.columns]\n",
    "    preview_df = orders_silver.sample(False, sample_frac, seed=42).select(*orders_preview_cols)\n",
    "    if preview_df.rdd.isEmpty():\n",
    "        preview_df = orders_silver.select(*orders_preview_cols).limit(preview_rows)\n",
    "    preview_df.show(preview_rows, truncate=False)\n",
    "\n",
    "    print(\"\\nPreview users_silver (primeiras linhas):\")\n",
    "    users_preview_cols = [c for c in [\n",
    "        \"customer_id\",\"last_order\",\"frequency\",\"monetary\",\"is_target\",\"recency\"\n",
    "    ] if c in users_silver.columns]\n",
    "    users_silver.select(*users_preview_cols).show(preview_rows, truncate=False)\n",
    "\n",
    "    if use_pandas_preview:\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(orders_silver.limit(preview_rows).toPandas())\n",
    "            display(users_silver.limit(preview_rows).toPandas())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- Executar em modo leve ---\n",
    "check_post_etl(orders_silver, users_silver, light=True, check_semantic_dups=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db6755",
   "metadata": {},
   "source": [
    "## A/B de cupons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d1372",
   "metadata": {},
   "source": [
    "### Importações e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0ecbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from src.analysis_ab import (\n",
    "    compute_ab_summary,\n",
    "    compute_robust_metrics,\n",
    "    collect_user_level_for_tests,\n",
    "    run_ab_tests,\n",
    "    run_nonparam_tests,\n",
    "    financial_viability\n",
    ")\n",
    "from src.utils import load_settings\n",
    "from src.analysis_ab import compute_ab_summary, collect_user_level_for_tests, compute_robust_metrics\n",
    "from src.viz_ab import plot_group_bars, plot_ab_box, plot_ab_hist_overlay, save_table_csv\n",
    "\n",
    "settings = load_settings(\"config/settings.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62f8c6",
   "metadata": {},
   "source": [
    "### Visualizações para exploração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d6442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Amostragem controlada já no Spark (mais leve no driver)\n",
    "#    - diminua sample_frac se ainda ficar pesado\n",
    "#    - max_rows garante teto absoluto no tamanho coletado\n",
    "# ---------------------------\n",
    "SAMPLE_FRAC = 0.15     # 10–20% costuma bastar para ver a assimetria\n",
    "MAX_ROWS    = 100_000  # teto duro (evita coletar demais)\n",
    "CLIP_P      = 0.01     # recorta 1% de cada cauda para visual\n",
    "\n",
    "users_pdf = collect_user_level_for_tests(\n",
    "    users_silver.select(\"user_id\",\"is_target\",\"monetary\",\"frequency\"),  \n",
    "    sample_frac=SAMPLE_FRAC,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# teto de linhas no pandas (caso a fração ainda gere um DF muito grande)\n",
    "if len(users_pdf) > MAX_ROWS:\n",
    "    users_pdf = users_pdf.sample(n=MAX_ROWS, random_state=42)\n",
    "\n",
    "# AOV (se já não vier calculado)\n",
    "if \"aov_user\" not in users_pdf.columns:\n",
    "    users_pdf[\"aov_user\"] = users_pdf[\"monetary\"] / users_pdf[\"frequency\"].replace({0: np.nan})\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Recorte de cauda para visual (não altera dados de análise “oficial”)\n",
    "# ---------------------------\n",
    "q_low  = users_pdf[[\"monetary\",\"frequency\",\"aov_user\"]].quantile(CLIP_P)\n",
    "q_high = users_pdf[[\"monetary\",\"frequency\",\"aov_user\"]].quantile(1-CLIP_P)\n",
    "for col in [\"monetary\",\"frequency\",\"aov_user\"]:\n",
    "    users_pdf[col] = users_pdf[col].clip(q_low[col], q_high[col])\n",
    "\n",
    "# labels legíveis\n",
    "users_pdf[\"grupo\"] = users_pdf[\"is_target\"].map({0:\"Controle\", 1:\"Tratamento\"}).astype(\"category\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Boxplots (com showfliers=False para não pesar na renderização)\n",
    "# ---------------------------\n",
    "sns.set_theme()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.boxplot(x=\"grupo\", y=\"monetary\", data=users_pdf, ax=axes[0], showfliers=False)\n",
    "axes[0].set_title(\"GMV por Usuário (amostra, caudas recortadas)\")\n",
    "\n",
    "sns.boxplot(x=\"grupo\", y=\"frequency\", data=users_pdf, ax=axes[1], showfliers=False)\n",
    "axes[1].set_title(\"Pedidos por Usuário (amostra)\")\n",
    "\n",
    "sns.boxplot(x=\"grupo\", y=\"aov_user\", data=users_pdf, ax=axes[2], showfliers=False)\n",
    "axes[2].set_title(\"AOV por Usuário (amostra)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Barras de médias (na amostra) — registro do \"primeiro olhar\"\n",
    "# ---------------------------\n",
    "metrics_means = (users_pdf\n",
    "                 .groupby(\"grupo\")[[\"monetary\",\"frequency\",\"aov_user\"]]\n",
    "                 .mean()\n",
    "                 .reset_index())\n",
    "ax = metrics_means.plot(x=\"grupo\", kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Médias por Grupo (GMV, Pedidos, AOV) — amostra\")\n",
    "plt.ylabel(\"Valor médio (amostra)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Histogramas (sobrepostos) com caudas recortadas\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(users_pdf.loc[users_pdf[\"grupo\"]==\"Controle\",\"frequency\"],\n",
    "             bins=30, stat=\"density\", alpha=0.5, label=\"Controle\")\n",
    "sns.histplot(users_pdf.loc[users_pdf[\"grupo\"]==\"Tratamento\",\"frequency\"],\n",
    "             bins=30, stat=\"density\", alpha=0.5, label=\"Tratamento\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribuição de Pedidos por Usuário — amostra (caudas recortadas)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (opcional) cache da amostra para reusar na sessão\n",
    "# users_pdf.to_parquet(\"outputs/cache/users_pdf_sample.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec43eae",
   "metadata": {},
   "source": [
    "### Métricas por grupo\n",
    "\n",
    "Premissas:\n",
    "* Valor do cupom: R$ 10,00\n",
    "    *  Pago integralmente pelo iFood\n",
    "* Taxa de conversão: 25%\n",
    "* Take rate: 23%\n",
    "\n",
    "Dada a distribuição assimétrica dos dados, com muitos outliers, evidenciada pelos gráficos, optou-se por utilizar métricas robustas (medianas, p95, heavy users) para a análise de impacto. Métricas robustas ajudam a evitar decisões enviesadas por outliers, garantindo que o ROI/LTV:CAC seja interpretado à luz do comportamento da maioria dos usuários. Métricas baseadas em médias também são apresentadas para comparação, mas com cautela, pois podem ser influenciadas por valores extremos. O relatório final incluirá somente métricas robustas e testes estatísticos apropriados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39c78a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "out_ab = \"outputs/ab\"\n",
    "\n",
    "# Resumo por grupo (descrição em Spark)\n",
    "ab_summary_spark = compute_ab_summary(users_silver)\n",
    "ab_summary_spark.show(truncate=False)\n",
    "ab_summary_pdf = ab_summary_spark.toPandas()\n",
    "save_table_csv(ab_summary_pdf, out_ab, \"ab_means\")\n",
    "\n",
    "# Métricas robustas (mediana, p95, heavy)\n",
    "robust_spark = compute_robust_metrics_spark(users_silver, heavy_threshold=3)\n",
    "robust_pdf   = robust_spark.toPandas()\n",
    "display(robust_pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aa7cc",
   "metadata": {},
   "source": [
    "### Testes de significância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501af56c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Teste paramétrico\n",
    "ttest_out = run_ab_tests(users_pdf) \n",
    "\n",
    "# Teste não-paramétrico\n",
    "mw_out    = run_nonparam_tests(users_pdf)  \n",
    "\n",
    "print(\"Welch t-test:\", ttest_out)\n",
    "print(\"Mann–Whitney:\", mw_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54476f50",
   "metadata": {},
   "source": [
    "### Viabilidade financeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffdc1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import load_settings\n",
    "s = load_settings()\n",
    "\n",
    "finance = financial_viability(\n",
    "    users_pdf,\n",
    "    take_rate=s.finance.take_rate,                \n",
    "    coupon_cost=s.finance.coupon_cost_default,    \n",
    "    redemption_rate=0.30\n",
    ")\n",
    "finance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6273ac",
   "metadata": {},
   "source": [
    "### Visualizações para relatório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01590dca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.viz_ab import plot_group_bars, plot_ab_box, plot_ab_hist_overlay\n",
    "\n",
    "# barras com médias\n",
    "plot_group_bars(\n",
    "    ab_summary_pdf.rename(columns={\"aov\":\"aov_user\"}),\n",
    "    metrics=[\"gmv_user\",\"pedidos_user\",\"aov_user\"],\n",
    "    labels_map={\"gmv_user\":\"GMV/usuário\",\"pedidos_user\":\"Pedidos/usuário\",\"aov_user\":\"AOV\"},\n",
    "    outdir=\"outputs/ab\", fname=\"bars_means\", title=\"Médias por grupo\"\n",
    ")\n",
    "\n",
    "# barras com medianas (robust_pdf vindo do Spark)\n",
    "plot_group_bars(\n",
    "    robust_pdf.rename(columns={\n",
    "        \"median_gmv_user\":\"GMV mediano\",\n",
    "        \"median_pedidos_user\":\"Pedidos medianos\",\n",
    "        \"median_aov_user\":\"AOV mediano\"\n",
    "    })[[\"is_target\",\"GMV mediano\",\"Pedidos medianos\",\"AOV mediano\"]],\n",
    "    metrics=[\"GMV mediano\",\"Pedidos medianos\",\"AOV mediano\"],\n",
    "    outdir=\"outputs/ab\", fname=\"bars_medians\", title=\"Métricas robustas por grupo\"\n",
    ")\n",
    "\n",
    "# distribuições com users_pdf (amostra)\n",
    "for metric in [\"monetary\",\"frequency\",\"aov_user\"]:\n",
    "    plot_ab_box(users_pdf, metric, outdir=\"outputs/ab\", fname=f\"box_{metric}\", clip_p=0.01)\n",
    "    plot_ab_hist_overlay(users_pdf, metric, outdir=\"outputs/ab\", fname=f\"hist_{metric}\", clip_p=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7587c",
   "metadata": {},
   "source": [
    "## Análise de Segmentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ff804",
   "metadata": {},
   "source": [
    "### Construção de segmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72251b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.analysis_segments import build_rfm_buckets, ab_metrics_by_segment\n",
    "from src.viz_segments import to_pandas_spark, save_table_csv\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# RFM\n",
    "users_with_rfm = build_rfm_buckets(users_silver)\n",
    "\n",
    "sample_ids_spark = spark.createDataFrame(\n",
    "    users_pdf[[\"customer_id\"]].drop_duplicates()\n",
    ")\n",
    "rfm_small = (\n",
    "    users_with_rfm\n",
    "    .join(sample_ids_spark, \"customer_id\", \"inner\")\n",
    "    .select(\"customer_id\",\"rfm_segment\")\n",
    "    .toPandas()\n",
    ")\n",
    "users_pdf = users_pdf.merge(rfm_small, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Heavy / New / Platform / RFM\n",
    "ab_heavy = ab_metrics_by_segment(users_silver, segment_col=\"heavy_user\")\n",
    "ab_new   = ab_metrics_by_segment(users_silver, segment_col=\"is_new_customer\")\n",
    "ab_plat  = ab_metrics_by_segment(users_silver, segment_col=\"origin_platform\")\n",
    "ab_rfm   = ab_metrics_by_segment(users_with_rfm, segment_col=\"rfm_segment\", top_k_segments=10)\n",
    "\n",
    "ab_heavy_pd = to_pandas_spark(ab_heavy)\n",
    "ab_new_pd   = to_pandas_spark(ab_new)\n",
    "ab_plat_pd  = to_pandas_spark(ab_plat)\n",
    "ab_rfm_pd   = to_pandas_spark(ab_rfm)\n",
    "\n",
    "outdir = \"outputs/segments\"\n",
    "save_table_csv(ab_heavy_pd, outdir, \"ab_heavy_summary\")\n",
    "save_table_csv(ab_new_pd,   outdir, \"ab_new_summary\")\n",
    "save_table_csv(ab_plat_pd,  outdir, \"ab_platform_summary\")\n",
    "save_table_csv(ab_rfm_pd,   outdir, \"ab_rfm_summary\")\n",
    "ab_heavy_pd, ab_new_pd.head(), ab_plat_pd.head(), ab_rfm_pd.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe798458",
   "metadata": {},
   "source": [
    "### Visualização de Segmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2ef14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.viz_segments import plot_bars_by_segment, plot_box_by_segment, plot_hist_by_segment\n",
    "\n",
    "figdir = \"outputs/figs_segments\"\n",
    "\n",
    "# 1) Barras de médias por segmento (GMV/user, pedidos/user, AOV)\n",
    "metrics_cols = [\"gmv_user\", \"pedidos_user\", \"aov\"]\n",
    "plot_bars_by_segment(ab_heavy_pd, \"heavy_user\", metrics_cols, title=\"Médias por segmento\", outdir=figdir, fname=\"bars_heavy\")\n",
    "plot_bars_by_segment(ab_new_pd,   \"is_new_customer\", metrics_cols, title=\"Médias por segmento\", outdir=figdir, fname=\"bars_new\")\n",
    "plot_bars_by_segment(ab_plat_pd,  \"origin_platform\", metrics_cols, title=\"Médias por segmento\", outdir=figdir, fname=\"bars_platform\")\n",
    "\n",
    "# 2) Boxplots por segmento (com e sem clipping p/ reduzir outliers visuais)\n",
    "for metric in [\"monetary\", \"frequency\", \"aov_user\"]:\n",
    "    plot_box_by_segment(users_pdf, \"heavy_user\", metric, clip_p=None, title=f\"Boxplot {metric} por heavy_user\", outdir=figdir, fname=f\"box_heavy\")\n",
    "    plot_box_by_segment(users_pdf, \"heavy_user\", metric, clip_p=0.01, title=f\"Boxplot {metric} por heavy_user (winsor 1%)\", outdir=figdir, fname=f\"box_heavy_w\")\n",
    "\n",
    "# 3) Histogramas por segmento (distribuições lado a lado, com clipping opcional)\n",
    "for metric in [\"monetary\", \"frequency\", \"aov_user\"]:\n",
    "    plot_hist_by_segment(users_pdf, \"is_new_customer\", metric, bins=40, clip_p=0.01, title=f\"Hist {metric} por is_new_customer\", outdir=figdir, fname=f\"hist_new\")\n",
    "    plot_hist_by_segment(users_pdf, \"origin_platform\", metric, bins=40, clip_p=0.01, title=f\"Hist {metric} por origin_platform\", outdir=figdir, fname=f\"hist_platform\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f8d84",
   "metadata": {},
   "source": [
    "### Testes e métricas robustas por segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7ed5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.analysis_ab import collect_user_level_for_tests\n",
    "from src.analysis_segments import robust_metrics_by_segment, nonparam_tests_by_segment, finance_by_segment\n",
    "\n",
    "SEG_COLS = [c for c in [\"heavy_user\",\"is_new_customer\",\"origin_platform\",\"rfm_segment\"] if c in users_pdf.columns]\n",
    "\n",
    "robust_tables = {}\n",
    "mw_tests = {}\n",
    "finance_tables = {}\n",
    "\n",
    "for seg_col in SEG_COLS:\n",
    "    robust_tables[seg_col]  = robust_metrics_by_segment(users_pdf, segment_col=seg_col, heavy_threshold=3)\n",
    "    mw_tests[seg_col]       = nonparam_tests_by_segment(users_pdf, segment_col=seg_col)\n",
    "    finance_tables[seg_col] = finance_by_segment(users_pdf, segment_col=seg_col, take_rate=0.23, coupon_cost=10.0, redemption_rate=0.30)\n",
    "\n",
    "# Imprimir exemplo\n",
    "display(robust_tables[\"heavy_user\"])\n",
    "mw_tests[\"heavy_user\"], list(finance_tables[\"heavy_user\"].items())[:2]\n",
    "\n",
    "for seg_col, df in robust_tables.items():\n",
    "    save_table_csv(df, outdir, f\"robust_{seg_col}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
