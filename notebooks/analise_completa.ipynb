{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe50670",
   "metadata": {},
   "source": [
    "# Análise Completa - Case Ifood: Teste A/B Estratégia de Cupons\n",
    "\n",
    "Notebook **único** para executar o *setup* (Colab ou local), **ETL** e análise dos dados.\n",
    "\n",
    "- Clona/atualiza o repositório (no Colab)\n",
    "- Instala dependências e faz o **download** dos dados brutos\n",
    "- Sobe Spark e executa o **ETL** (orders/consumers/restaurants + mapa A/B)\n",
    "- Mantém `orders_silver` e `users_silver` em memória\n",
    "- Realiza a análise exploratória dos dados\n",
    "\n",
    "> Dica: por padrão **não** salva Parquet (evita `winutils.exe` no Windows). Se quiser materializar, ative `SAVE_PARQUET=True` mais abaixo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc5c4c",
   "metadata": {},
   "source": [
    "## COnfiguração de Ambiente e Download de Dados Brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daddc1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Bootstrap (Colab + Local) ---\n",
    "import os, sys, subprocess, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "GITHUB_USER = \"silvaniacorreia\"\n",
    "REPO_NAME   = \"ifood-case-cupons\"\n",
    "REPO_URL    = f\"https://github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n",
    "\n",
    "IN_COLAB = \"COLAB_RELEASE_TAG\" in os.environ or \"COLAB_GPU\" in os.environ\n",
    "PERSIST_ON_DRIVE = False  \n",
    "\n",
    "def run(cmd):\n",
    "    print(\">\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def find_project_root(start: Path = None) -> Path:\n",
    "    start = start or Path.cwd().resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"requirements.txt\").exists() and (p / \"src\" / \"utils.py\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1) clonar/atualizar repo\n",
    "    CONTENT_DIR = Path(\"/content\")\n",
    "    PROJECT_DIR = CONTENT_DIR / REPO_NAME\n",
    "    if not PROJECT_DIR.exists():\n",
    "        run([\"git\", \"clone\", REPO_URL, str(PROJECT_DIR)])\n",
    "    else:\n",
    "        os.chdir(PROJECT_DIR)\n",
    "        run([\"git\", \"fetch\", \"--all\"])\n",
    "        run([\"git\", \"checkout\", \"main\"])\n",
    "        run([\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"])\n",
    "    os.chdir(PROJECT_DIR)\n",
    "\n",
    "    # 2) deps + download \n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    run([sys.executable, \"scripts/download_data.py\"])\n",
    "\n",
    "    # 3) sys.path\n",
    "    if str(PROJECT_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_DIR))\n",
    "    print(\"✔️ Bootstrap concluído (Colab). Projeto:\", PROJECT_DIR)\n",
    "\n",
    "else:\n",
    "    PROJECT_DIR = find_project_root(Path.cwd().resolve())\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    if str(PROJECT_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_DIR))\n",
    "    print(\"Execução local. Raiz do projeto:\", PROJECT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f4bcb",
   "metadata": {},
   "source": [
    "## Teste Smoke do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66ab3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils import load_settings, get_spark\n",
    "\n",
    "s = load_settings()\n",
    "spark = get_spark(\n",
    "    app_name=s.runtime.spark.app_name,\n",
    "    shuffle_partitions=s.runtime.spark.shuffle_partitions\n",
    ")\n",
    "print(\"✔️ Spark ativo - versão:\", spark.version)\n",
    "\n",
    "# checagem rápida\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc2464",
   "metadata": {},
   "source": [
    "## ETL end-to-end (mantém DataFrames em memória) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1918c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src import etl\n",
    "from pyspark.sql import functions as F\n",
    "import importlib, os\n",
    "\n",
    "# 1) ler brutos\n",
    "orders, consumers, restaurants, abmap = etl.load_raw(spark, s.data.raw_dir)\n",
    "\n",
    "# 2) parâmetros da análise (do settings, se houver)\n",
    "win = getattr(s.analysis, \"experiment_window\", None)\n",
    "start = win.get(\"start\") if isinstance(win, dict) else None\n",
    "end   = win.get(\"end\")   if isinstance(win, dict) else None\n",
    "auto  = getattr(s.analysis, \"auto_infer_window\", True)\n",
    "\n",
    "# 3) conform + joins + janela\n",
    "df = etl.clean_and_conform(\n",
    "    orders, consumers, restaurants, abmap,\n",
    "    business_tz=getattr(s.analysis, \"business_tz\", \"America/Sao_Paulo\"),\n",
    "    treat_is_target_null_as_control=getattr(s.analysis, \"treat_is_target_null_as_control\", False),\n",
    "    experiment_start=start,\n",
    "    experiment_end=end,\n",
    "    auto_infer_window=auto,\n",
    ")\n",
    "\n",
    "# 4) silvers em memória\n",
    "orders_silver = etl.build_orders_silver(df)\n",
    "users_silver  = etl.build_user_aggregates(orders_silver)\n",
    "\n",
    "# recency com base no último timestamp observado\n",
    "ref_ts = orders_silver.agg(F.max(\"event_ts_utc\")).first()[0]\n",
    "users_silver = users_silver.withColumn(\n",
    "    \"recency\",\n",
    "    F.when(F.col(\"last_order\").isNotNull(), F.datediff(F.lit(ref_ts), F.col(\"last_order\")))\n",
    ")\n",
    "\n",
    "# 5) (opcional) salvar parquet localmente\n",
    "SAVE_PARQUET = True  \n",
    "if SAVE_PARQUET:\n",
    "    orders_silver.write.mode(\"overwrite\").parquet(f\"{s.data.processed_dir}/orders_silver.parquet\")\n",
    "    users_silver.write.mode(\"overwrite\").parquet(f\"{s.data.processed_dir}/users_silver.parquet\")\n",
    "\n",
    "print(\"orders_silver:\", orders_silver.count(), \"linhas\")\n",
    "print(\"users_silver :\", users_silver.count(), \"linhas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8d32d",
   "metadata": {},
   "source": [
    "## Checagem dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4388b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Checks essenciais (nulos, janela, split A/B) ---\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def nulls_by_col(df):\n",
    "    exprs = [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]\n",
    "    return df.select(exprs)\n",
    "\n",
    "print(\"Faixa de datas (UTC) em orders_silver:\")\n",
    "orders_silver.agg(F.min(\"event_ts_utc\").alias(\"min_utc\"),\n",
    "                  F.max(\"event_ts_utc\").alias(\"max_utc\")).show()\n",
    "\n",
    "print(\"Split A/B (users):\")\n",
    "users_silver.groupBy(\"is_target\").count().show()\n",
    "\n",
    "print(\"Nulos em orders_silver:\")\n",
    "nulls_by_col(orders_silver).show(truncate=False)\n",
    "\n",
    "# previews\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(orders_silver.limit(5).toPandas())\n",
    "    display(users_silver.limit(5).toPandas())\n",
    "except Exception:\n",
    "    print(\"Preview (head) orders_silver:\", orders_silver.limit(5).toPandas().head())\n",
    "    print(\"Preview (head) users_silver :\", users_silver.limit(5).toPandas().head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
