# iFood Case â€“ Cupons

Pipeline reprodutÃ­vel para o case de cupons do iFood: **download programÃ¡tico dos dados**, setup de **PySpark**, e notebooks para **ETL, RFM, A/B** e **ROI** (em construÃ§Ã£o).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/00_setup_and_checks.ipynb)

---

## âœ… Status (o que jÃ¡ estÃ¡ pronto)

- **Download programÃ¡tico** e extraÃ§Ã£o (`scripts/download_data.py`) lendo URLs do `config/settings.yaml`.
- **Bootstrap Colab/Local** no `notebooks/00_setup_and_checks.ipynb`:
  - Colab: clona o repositÃ³rio, instala dependÃªncias, baixa e prepara os dados.
  - Local: verifica paths e roda o mesmo script de download.
- **UtilitÃ¡rios** (`src/utils.py`): `load_settings`, `get_spark`, `set_seeds`, `stop_spark`.
- **Smoke test** do Spark no `00_setup_and_checks.ipynb`.

> PrÃ³ximas etapas (em construÃ§Ã£o): `src/etl.py`, `src/segments.py` (RFM), `src/abtest.py`, `src/finance.py` + notebooks `01_`, `02_`, `03_`.

---

## ğŸ§© Estrutura

ifood-case-cupons/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ notebooks/
â”‚  â”œâ”€ 00_setup_and_checks.ipynb      # bootstrap + download programÃ¡tico + smoke do Spark
â”‚  â”œâ”€ 01_etl_pyspark.ipynb           # (em construÃ§Ã£o)
â”‚  â”œâ”€ 02_abtest_and_segments.ipynb   # (em construÃ§Ã£o)
â”‚  â””â”€ 03_financial_roi.ipynb         # (em construÃ§Ã£o)
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ utils.py                       # settings + spark + seeds
â”œâ”€ scripts/
â”‚  â””â”€ download_data.py               # baixa .gz/.tar.gz e extrai o .tar.gz
â”œâ”€ config/
â”‚  â”œâ”€ settings.example.yaml
â”‚  â””â”€ settings.yaml                  # contÃ©m o bloco `sources:` com as URLs
â”œâ”€ data/
â”‚  â”œâ”€ raw/                           # arquivos baixados (spark lÃª .gz direto)
â”‚  â””â”€ processed/                     # saÃ­das intermediÃ¡rias (parquet)
â””â”€ report/

## âš™ï¸ ConfiguraÃ§Ã£o

Arquivo: `config/settings.yaml`

ParÃ¢metros do projeto (seed, spark, caminhos) e bloco `sources` com as URLs e nomes dos 4 arquivos do case:

```yaml
sources:
  orders:      { url: ".../order.json.gz",      filename: "order.json.gz" }
  consumers:   { url: ".../consumer.csv.gz",    filename: "consumer.csv.gz" }
  restaurants: { url: ".../restaurant.csv.gz",  filename: "restaurant.csv.gz" }
  ab_test_ref: { url: ".../ab_test_ref.tar.gz", filename: "ab_test_ref.tar.gz" }
```

O `scripts/download_data.py` lÃª essas entradas, baixa os arquivos para `data/raw/` e extrai o `ab_test_ref.tar.gz` para `data/raw/ab_test_ref_extracted/`. Os `.gz` (JSON/CSV) nÃ£o precisam ser descompactados para o Spark.

---

## â–¶ï¸ Como executar no Colab (avaliadores)

Clique no badge acima (â€œOpen in Colabâ€).

No Colab, vÃ¡ em `Runtime â†’ Run all (Executar tudo)`.

A primeira cÃ©lula:
- clona este repositÃ³rio para `/content/ifood-case-cupons`,
- instala dependÃªncias do `requirements.txt`,
- roda `scripts/download_data.py` (download programÃ¡tico + extraÃ§Ã£o).

A segunda cÃ©lula faz o smoke test do Spark (deve imprimir â€œSpark OK: x.y.zâ€ e exibir uma tabela 0..4).

ApÃ³s isso, os dados estarÃ£o em:

```
/content/ifood-case-cupons/data/raw/
/content/ifood-case-cupons/data/raw/ab_test_ref_extracted/
```

Quando os notebooks seguintes forem concluÃ­dos, basta abrir `01_`, `02_` e `03_` e executar normalmente.

**ObservaÃ§Ã£o:** para que o Colab consiga clonar via `git clone`, o repositÃ³rio precisa estar pÃºblico.
Se ele estiver privado, hÃ¡ duas opÃ§Ãµes:

1. Tornar pÃºblico durante a avaliaÃ§Ã£o, ou
2. Abrir o notebook pelo menu `File â†’ Open notebook â†’ GitHub`, fazer login no GitHub e marcar â€œInclude private reposâ€. Neste caso, se o clone falhar por permissÃ£o, o avaliador pode baixar o ZIP do repositÃ³rio pelo GitHub e subir/arrastar para o Colab (pasta `/content/`), depois `cd /content/ifood-case-cupons` e rodar as cÃ©lulas normalmente.

---

## ğŸ’» Como executar localmente (desenvolvedores)

PrÃ©-requisitos: Python 3.10+, JDK 11+ (para Spark).

```bash
# 1) Ambiente
python -m venv .venv && source .venv/bin/activate      # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m ipykernel install --user --name ifood-case

# 2) Download programÃ¡tico
python scripts/download_data.py

# 3) Jupyter
jupyter notebook   # ou jupyter lab
# Abra notebooks/00_setup_and_checks.ipynb e rode "Run all"
```

Se aparecer erro de Java local, instale um JDK 11+ e confira `java -version` no terminal.

---

## ğŸ§ª Teste rÃ¡pido (smoke do Spark)

No `notebooks/00_setup_and_checks.ipynb` jÃ¡ existe uma cÃ©lula que faz:

```python
from src.utils import load_settings, set_seeds, get_spark, stop_spark
s = load_settings()
set_seeds(s.runtime.seed)
spark = get_spark(app_name=s.runtime.spark.app_name,
                  shuffle_partitions=s.runtime.spark.shuffle_partitions)
print("Spark OK:", spark.version)
spark.range(5).show()
stop_spark(spark)
```

---

## ğŸ”œ Roadmap (prÃ³ximas entregas)

- `src/etl.py` + `notebooks/01_etl_pyspark.ipynb`
- `src/segments.py` (RFM) + `notebooks/02_abtest_and_segments.ipynb`
- `src/abtest.py` (A/B com CUPED) + `src/finance.py` (ROI) + `notebooks/03_financial_roi.ipynb`

