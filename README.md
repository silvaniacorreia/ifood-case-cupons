# iFood Case â€“ Cupons

RepositÃ³rio para o case de **Analista de Dados** do iFood. Objetivo: analisar um **teste A/B** de uma estratÃ©gia de **cupons** com foco em **retenÃ§Ã£o** e crescimento, seguindo a ordem e escopo pedidos no case.

---

## âœ¨ Escopo do Case (ordem de entrega)

1) **Teste A/B (Campanha de Cupons)**
   - **(a)** Definir **mÃ©tricas de sucesso** e analisar **impacto estatisticamente significativo** no perÃ­odo avaliado.
   - **(b)** Realizar **anÃ¡lise de viabilidade financeira** (ex.: ROI / payback), deixando **premissas explÃ­citas**.
   - **(c)** Recomendar **oportunidades de melhoria** e **desenhar um novo A/B** (desenho experimental, mÃ©tricas e guardrails).

2) **SegmentaÃ§Ã£o de UsuÃ¡rios**
   - **(a)** Estabelecer **critÃ©rios/regras** dos segmentos (neste projeto: **RFM** como baseline), explicando o racional.
   - **(b)** Analisar **resultados do A/B por segmento** e **propor aÃ§Ãµes** especÃ­ficas.

3) **RecomendaÃ§Ãµes e PrÃ³ximos Passos**
   - Plano com **previsÃ£o de impacto** (financeiro ou nÃ£o) para as **LideranÃ§as de NegÃ³cio**.
   - **Melhorias de processo/teste** e **estratÃ©gias por segmento**.
   - **Premissas documentadas** quando necessÃ¡rio.

---

## ğŸš€ Status (atÃ© agora)

- **Bootstrap reprodutÃ­vel** (local e Colab) no `00_setup_and_checks.ipynb`:
  - Colab: clona o repo, instala deps, baixa e prepara os dados (**download programÃ¡tico**).
  - Local: garante paths e roda o mesmo script de download.
- **Download programÃ¡tico** + extraÃ§Ã£o: `scripts/download_data.py`
  - Baixa os 4 insumos de `config/settings.yaml`.
  - **Extrai** apenas o `ab_test_ref.tar.gz` (o Spark lÃª `.gz` diretamente).
  - **Ignora artefatos** do macOS `._*.csv` e `.DS_Store` ao localizar o CSV do A/B.
- **ETL base** (`01_etl_pyspark.ipynb` + `src/etl.py`):
  - **Orders**: tipagem, normalizaÃ§Ã£o de **timezone** (UTC canÃ´nico; BRT para â€œdia de negÃ³cioâ€), remoÃ§Ã£o/hasheamento de **PII**, validaÃ§Ãµes (duplicados, valores, geolocalizaÃ§Ã£o), escolha do **timestamp do evento** (agendado vs criado).
  - **Consumers / Restaurants**: tipagem, normalizaÃ§Ã£o, remoÃ§Ã£o/hasheamento de PII e renomeaÃ§Ãµes para joins seguros.
  - **Janela do experimento**: parametrizada via `settings.yaml` **ou** **inferida automaticamente** dos dados quando nÃ£o definida.
  - SaÃ­das: `data/processed/orders_silver.parquet` (fato por pedido) e `data/processed/users_silver.parquet` (R/F/M por usuÃ¡rio).
- **Checks de qualidade** no final do notebook (nulos, unicidade, janelas, split A/B, distribuiÃ§Ãµes).

> PrÃ³ximas etapas (em construÃ§Ã£o): `02_abtest_core.ipynb` (mÃ©tricas + testes), `03_financial_viability.ipynb` (ROI) e `04_segments_ab_by_segment.ipynb` (RFM + A/B por segmento).

---

## ğŸ“ Estrutura do repositÃ³rio

```
ifood-case-cupons/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ notebooks/
â”‚  â”œâ”€ 00_setup_and_checks.ipynb      # bootstrap + download programÃ¡tico + smoke Spark
â”‚  â”œâ”€ 01_etl_pyspark.ipynb           # ETL e silvers (orders/users)
â”‚  â”œâ”€ 02_abtest_core.ipynb           # (em construÃ§Ã£o) A/B mÃ©tricas + significÃ¢ncia
â”‚  â”œâ”€ 03_financial_viability.ipynb   # (em construÃ§Ã£o) ROI e sensibilidade
â”‚  â””â”€ 04_segments_ab_by_segment.ipynb# (em construÃ§Ã£o) RFM + leitura do A/B por segmento
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ utils.py                       # settings + spark + seeds
â”‚  â””â”€ etl.py                         # ingestÃ£o + limpeza + joins + silvers
â”œâ”€ scripts/
â”‚  â””â”€ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
â”œâ”€ config/
â”‚  â”œâ”€ settings.example.yaml
â”‚  â””â”€ settings.yaml                  # fontes + parÃ¢metros (ver abaixo)
â”œâ”€ data/
â”‚  â”œâ”€ raw/                           # arquivos baixados
â”‚  â””â”€ processed/                     # parquet gerados pelo ETL
â””â”€ report/                           # relatÃ³rio final (PDF)
```

---

## ğŸ”— Acesso rÃ¡pido aos Notebooks (abre em nova aba)

- **Setup & Checks**  
  <a href="https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/00_setup_and_checks.ipynb" target="_blank" rel="noopener">Abrir no Colab</a> Â·
  <a href="https://github.com/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/00_setup_and_checks.ipynb" target="_blank" rel="noopener">Ver no GitHub</a>

- **ETL (PySpark)**  
  <a href="https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/01_etl_pyspark.ipynb" target="_blank" rel="noopener">Abrir no Colab</a> Â·
  <a href="https://github.com/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/01_etl_pyspark.ipynb" target="_blank" rel="noopener">Ver no GitHub</a>

*(Os notebooks 02/03/04 aparecerÃ£o aqui quando forem versionados.)*

---

## âš™ï¸ ConfiguraÃ§Ã£o & ExecuÃ§Ã£o

### `config/settings.yaml` (exemplo mÃ­nimo)
```yaml
runtime:
  seed: 42
  spark:
    app_name: "ifood-case-cupons"
    shuffle_partitions: 64

data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"

sources:
  orders:      { url: "https://.../order.json.gz",      filename: "order.json.gz" }
  consumers:   { url: "https://.../consumer.csv.gz",    filename: "consumer.csv.gz" }
  restaurants: { url: "https://.../restaurant.csv.gz",  filename: "restaurant.csv.gz" }
  ab_test_ref: { url: "https://.../ab_test_ref.tar.gz", filename: "ab_test_ref.tar.gz" }

analysis:
  # Se NÃƒO definir a janela, o ETL **infere automaticamente** [min_data, max_data+1d) em UTC
  # experiment_window:
  #   start: "YYYY-MM-DD"
  #   end:   "YYYY-MM-DD"   # exclusivo
  business_tz: "America/Sao_Paulo"
  auto_infer_window: true
  treat_is_target_null_as_control: false
  winsorize: 0.02
  use_cuped: true
```

### ExecuÃ§Ã£o no **Colab** (avaliadores)
1. Abra **cada notebook** pelos links acima (o Colab abre em nova aba).  
2. Em `00_setup_and_checks`: **Runtime â†’ Run all**. A primeira cÃ©lula clona o repo, instala deps e roda o **download programÃ¡tico**; a segunda faz o **smoke** do Spark.  
3. Em `01_etl_pyspark`: **Run all** para gerar `orders_silver.parquet` e `users_silver.parquet` em `data/processed/`.


### ExecuÃ§Ã£o **local** (desenvolvedores)
PrÃ©-requisitos: **Python 3.10+** e **JDK 11+** (Spark).

```bash
# 1) Ambiente
python -m venv .venv && . .venv/Scripts/activate  # Windows PowerShell
# ou: source .venv/bin/activate                    # macOS/Linux
pip install -r requirements.txt
python -m ipykernel install --user --name ifood-case

# 2) Download programÃ¡tico
python scripts/download_data.py

# 3) Jupyter
jupyter notebook  # ou jupyter lab
# Abra 00_setup_and_checks.ipynb (Run all) e depois 01_etl_pyspark.ipynb (Run all)
```

> **Windows (importante):** para **escrever Parquet** com Spark, configure o **winutils.exe** (Hadoop):
> - Descubra a versÃ£o do Hadoop usada pelo Spark:
>   ```python
>   print("Spark:", spark.version)
>   hadoop_ver = spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()
>   print("Hadoop:", hadoop_ver)
>   ```
> - Instale o `winutils.exe` da **mesma linha de versÃ£o** (por ex., `C:\hadoop\hadoop-3.3.4\bin\winutils.exe`) e exporte:
>   ```powershell
>   $env:HADOOP_HOME="C:\hadoop\hadoop-3.3.4"
>   $env:PATH="$env:HADOOP_HOMEin;$env:PATH"
>   ```
> Reinicie o Jupyter apÃ³s configurar.

---

## ğŸ§± Premissas & DecisÃµes de Projeto

- **Timezone**: timestamps normalizados para **UTC** (`event_ts_utc`); para relatÃ³rios diÃ¡rios, usamos **BRT** (`event_date_brt`).  
- **Evento do pedido**: se `order_scheduled==true` **e** `order_scheduled_date` existir, usamos a data agendada; caso contrÃ¡rio, a data de criaÃ§Ã£o do pedido.  
- **Janela do experimento**: se `analysis.experiment_window` estiver **ausente**, o ETL **infere** automaticamente `[min(data), max(data)+1d)` em UTC e aplica o filtro.  
- **A/B sem marcaÃ§Ã£o**: `treat_is_target_null_as_control=false` por padrÃ£o (linhas sem `is_target` sÃ£o excluÃ­das, para evitar viÃ©s).  
- **PII**: `cpf` e telefone sÃ£o **hasheados**; nome e endereÃ§o sÃ£o **removidos** nas camadas analÃ­ticas.  
- **Leitura do A/B (CSV dentro de tar.gz)**: ignoramos arquivos â€œfantasmaâ€ do macOS (`._*.csv`, `.DS_Store`) e escolhemos o **maior CSV vÃ¡lido**.  
- **Outliers**: estatÃ­sticas de cauda sÃ£o inspecionadas no ETL; a decisÃ£o de **winsorizar** (ex.: 1â€“99%) serÃ¡ aplicada no notebook do A/B.  
- **Reprodutibilidade**: o **download Ã© programÃ¡tico** e o pipeline Ã© executÃ¡vel **no Colab** sem setup manual.

---

## ğŸ§ª Checks rÃ¡pidos no ETL (no final do `01_etl_pyspark`)

- Unicidade: `order_id` e `customer_id` distintos nas silvers.  
- Nulos por coluna (especial atenÃ§Ã£o a `event_ts_utc`, `order_total_amount`, `is_target`).  
- Faixa de datas (UTC) e contagem diÃ¡ria (BRT) por grupo (`is_target`).  
- Split do A/B (equilÃ­brio entre controle e tratamento).  
- Resumo/quantis de `order_total_amount` e inspeÃ§Ã£o de outliers.

---

## ğŸ—ºï¸ Roadmap (prÃ³ximas entregas)

- `02_abtest_core.ipynb`: mÃ©tricas por usuÃ¡rio (GMV/U, Pedidos/U, ConversÃ£o, AOV), **Welch t-test** e **z-test de proporÃ§Ãµes**; (opcional) **CUPED**.
- `03_financial_viability.ipynb`: **ROI** e anÃ¡lise de **sensibilidade** (take rate, custo do cupom, cobertura).
- `04_segments_ab_by_segment.ipynb`: **RFM** + leitura do **uplift por segmento** e recomendaÃ§Ãµes direcionadas.

---

## ğŸ“„ LicenÃ§a & Privacidade

- Dados **nÃ£o sÃ£o versionados**. O script `scripts/download_data.py` baixa os insumos a partir das URLs configuradas.  
- PII Ã© protegida nas **camadas analÃ­ticas** (hash/removidos).  
- RepositÃ³rio preparado para execuÃ§Ã£o em **ambiente gerenciado** (Colab) e **local** (com as observaÃ§Ãµes de Windows acima).
