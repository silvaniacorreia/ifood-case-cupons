# Case iFood: Teste A/B EstratÃ©gia de Cupons

RepositÃ³rio do case para **Analista de Dados** no iFood. Objetivo: analisar um **teste A/B** de cupons com foco em **retenÃ§Ã£o** e crescimento.

> ExecuÃ§Ã£o **100% no Google Colab** para mÃ¡xima reprodutibilidade (sem dependÃªncias locais).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

---

## ğŸ§­ Escopo da anÃ¡lise

1) **A/B de cupons**  
   (a) Definir **mÃ©tricas de sucesso** e checar **significÃ¢ncia** no perÃ­odo;  
   (b) **Viabilidade financeira** (ROI/payback) com **premissas explÃ­citas**;  
   (c) Recomendar **melhorias** e desenhar um **novo A/B** (mÃ©tricas/guardrails).

2) **SegmentaÃ§Ã£o de usuÃ¡rios**  
   (a) **RFM** como baseline (critÃ©rios e racional claros);  
   (b) Ler o **efeito do A/B por segmento** e propor **aÃ§Ãµes por pÃºblico**.

3) **PrÃ³ximos passos**  
   Estimativa de **impacto** (financeiro ou nÃ£o) e sugestÃµes de **processo/teste**.

---

## ğŸ—‚ï¸ Estrutura do repositÃ³rio

```
ifood-case-cupons/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ notebooks/
â”‚  â”œâ”€ pipeline_analise_completa.ipynb # notebook principal (orquestraÃ§Ã£o da anÃ¡lise)
â”œâ”€ src/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ utils.py                       # settings + spark + seeds
â”‚  â”œâ”€ etl.py                         # ingestÃ£o + limpeza + joins + silvers
â”‚  â””â”€ analysis_ab.py                 # funÃ§Ãµes de anÃ¡lise A/B
â”œâ”€ scripts/
â”‚  â””â”€ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
â”œâ”€ config/
â”‚  â””â”€ settings.yaml                  # fontes + parÃ¢metros (ver abaixo)
â”œâ”€ data/
â”‚  â”œâ”€ raw/                           # arquivos baixados
â”‚  â””â”€ processed/                     # parquet gerados pelo ETL
â””â”€ report/                           # relatÃ³rio final (PDF)
```

---

## ğŸ§± Arquitetura & otimizaÃ§Ãµes

### ETL otimizado
1. **Janela de anÃ¡lise antes do join (robusta a outliers)**  
   A janela Ã© **inferida automaticamente pelos quantis 1â€“99% de `event_ts_utc`** e aplicada **antes** dos joins.  
   **Impacto:** reduz shuffle, memÃ³ria e custo de join sem ser â€œpuxadaâ€ por timestamps anÃ´malos.

2. **ProjeÃ§Ã£o mÃ­nima de colunas**  
   Selecionamos apenas as colunas necessÃ¡rias antes dos joins.  
   **Impacto:** menos shuffle/memÃ³ria, joins e writes mais rÃ¡pidos.

3. **Reparticionamento por chave de join**  
   `orders` Ã© reparticionado por `customer_id` usando `spark.sql.shuffle.partitions`.  
   **Impacto:** melhor balanceamento no shuffle durante o join.

4. **Broadcast de dimensÃ£o pequena**  
   `restaurants` Ã© broadcast para habilitar broadcast-hash join.  
   **Impacto:** elimina shuffle dessa dimensÃ£o e acelera o join.

5. **Controles de verbosidade e cache**  
   `verbose=False` por padrÃ£o (evita `count()`/`show()` desnecessÃ¡rios) e `cache_intermediates=False` (reduz risco de OOM).

6. **Escrita em Parquet opcional e leve**  
   PersistÃªncia Ã© **opcional** (desligada por padrÃ£o). Ao salvar, usamos `partitionBy(event_date_brt)` e **coalesce** para evitar explosÃ£o de arquivos.

### ConfiguraÃ§Ãµes do Spark para Colab
- **AQE ligado:** `spark.sql.adaptive.enabled=true`
- **Serializer Kryo:** menor overhead de serializaÃ§Ã£o
- **Timezone fixa (UTC):** conversÃµes consistentes
- **`spark.sql.shuffle.partitions`:** 32 (definido por testes de benchmark) via `config/settings.yaml`

**Exemplo de `runtime.spark.conf` no Colab:**
```yaml
runtime:
  spark:
    app_name: "ifood-case-cupons"
    driver_memory: "12g"
    shuffle_partitions: 128
    conf:
      spark.master: "local[*]"
      spark.sql.adaptive.enabled: "true"
      spark.sql.adaptive.coalescePartitions.enabled: "true"
      spark.sql.files.maxPartitionBytes: "64m"
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"
      spark.memory.fraction: "0.6"
      spark.sql.autoBroadcastJoinThreshold: "50MB"
```

### Leituras e conformizaÃ§Ãµes robustas
- **Reader resiliente de orders:** detecta NDJSON ou JSON array e tem fallback com gzip/json.
- **SanitizaÃ§Ã£o de PII e tipagem:** normalizamos timestamps, lat/long, flags, calculamos `basket_size` de forma segura, e removemos/hasheamos PII.
- **Checagens prÃ©vias (preflight):** verificamos tamanhos, compressÃ£o e candidatos do `ab_test_ref` antes de acionar o Spark; falha cedo em caso de problema.

### Enriquecimentos para anÃ¡lise (camada â€œsilverâ€)
- **`origin_platform` nulo â†’ `"unknown"`** (evita perdas em cortes por canal).
- **Campos de consumidor faltantes**: versÃµes limpas para segmentaÃ§Ã£o (ex.: `language_clean = coalesce(language, 'unknown')`), mantendo os originais para auditoria.
- **Atributos de restaurante imputados (colunas paralelas)**  
  - `minimum_order_value_imputed`: mediana por (`merchant_city`, `price_range`) com fallback por `price_range`.  
  - `delivery_time_imputed`: mediana por `price_range`.  
  As colunas **originais sÃ£o preservadas**; as versÃµes imputadas sÃ£o usadas apenas para diagnÃ³stico/controle (ex.: balance check/CUPED).

---

## â–¶ï¸ Como executar

### ExecuÃ§Ã£o no Colab

1. Abra o notebook **no Colab**:  
   [**pipeline_analise_completa.ipynb**](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

2. **Runtime â†’ Run all**. A primeira cÃ©lula:
   - clona/atualiza o repositÃ³rio;
   - instala as dependÃªncias de `requirements.txt`;
   - roda o **download programÃ¡tico** (`scripts/download_data.py`);

3. O notebook entÃ£o executa:
   - **PrÃ©-flight** (fail-fast) dos arquivos baixados;  
   - **Profiling** dos 4 dataframes brutos;  
   - **ETL** completo com normalizaÃ§Ã£o de timezone/PII e joins;  
   - **A/B**, **viabilidade** e **RFM** (em sequÃªncia).

> **Tempo de execuÃ§Ã£o:** a leitura de `orders` (~1.6 GB gz) pode levar alguns minutos no Colab (gzip nÃ£o Ã© splittable). Depois da leitura, o ETL **reparticiona por `customer_id`** para paralelizar os joins.

---

## âš™ï¸ ConfiguraÃ§Ãµes importantes (`config/settings.yaml`)

| Caminho                         | DescriÃ§Ã£o |
|--------------------------------|-----------|
| `data.raw_dir`                 | Pasta dos brutos (default: `data/raw`) |
| `data.processed_dir`           | Pasta dos parquet (se habilitar salvar) |
| `runtime.spark.driver_memory`  | MemÃ³ria do driver no Colab (`12g`) |
| `analysis.business_tz`         | TZ de negÃ³cio (default `America/Sao_Paulo`) |
| `analysis.experiment_window`   | **auto-inferida** |
| `analysis.auto_infer_window`   | `true`/`false` â€” ativa a inferÃªncia de janela |
| `analysis.treat_is_target_null_as_control` | `false` por padrÃ£o (linhas sem grupo sÃ£o excluÃ­das) |
| `analysis.winsorize`/`use_cuped` | ParÃ¢metros para A/B (aplicados nas anÃ¡lises) |
| `runtime.spark.conf.*`        | Confs avanÃ§adas do Spark (AQE, Kryo, partiÃ§Ãµes, broadcast etc.) |

---

## ğŸ§± DecisÃµes tÃ©cnicas & otimizaÃ§Ãµes de desempenho

### Formato e leitura dos dados
- **Formato de `orders`**: detectado como **NDJSON**; leitura com `spark.read.json(...)`.  
  - Como o arquivo Ã© grande e gzip nÃ£o Ã© splittable, a leitura inicial roda em 1 task; apÃ³s ler, fazemos:  
    **`o = o.repartition(spark.sql.shuffle.partitions, 'customer_id')`** para **distribuir** o trabalho nos joins.
  - **Broadcast** de dimensÃµes: `restaurants` sempre (pequena) e `abmap` se `count â‰¤ 2M`.

### ConfiguraÃ§Ã£o do Spark
- **driver_memory**: ajustado para **12g** para evitar problemas de memÃ³ria.

### PersistÃªncia e formato de saÃ­da
- **PersistÃªncia estratÃ©gica**: usamos `.cache()` para evitar recomputaÃ§Ã£o em etapas subsequentes.  
- **Parquet**: apÃ³s o ETL, os DataFrames processados podem ser salvos em Parquet para acelerar leituras futuras.

---

## ğŸ§° O notebook como orquestrador tÃ©cnico

O notebook **pipeline_analise_completa.ipynb** funciona como um **orquestrador tÃ©cnico** das tarefas de anÃ¡lise, integrando os diferentes mÃ³dulos do repositÃ³rio:

1. **ConfiguraÃ§Ã£o inicial**:
   - Clona o repositÃ³rio e instala as dependÃªncias.
   - Faz o download programÃ¡tico dos dados brutos.

2. **ConfiguraÃ§Ã£o do Spark**:
   - LÃª as configuraÃ§Ãµes do arquivo `settings.yaml` e inicializa o Spark com parÃ¢metros ajustados para o Colab.

3. **ExecuÃ§Ã£o das tarefas**:
   - **PrÃ©-flight**: validaÃ§Ã£o dos arquivos brutos.
   - **ETL**: utiliza funÃ§Ãµes do mÃ³dulo `src/etl.py` para ingestÃ£o, conformaÃ§Ã£o e geraÃ§Ã£o dos DataFrames "silver".
   - **AnÃ¡lises**: executa mÃ©tricas de A/B, ROI e segmentaÃ§Ã£o, utilizando funÃ§Ãµes especÃ­ficas dos mÃ³dulos `src/utils.py` e `src/checks.py`.

4. **OrquestraÃ§Ã£o**:
   - O notebook organiza a execuÃ§Ã£o das etapas de forma sequencial, garantindo que cada tarefa seja realizada com base nos resultados da anterior.

---

## âœ… PrÃ©-flight & Profiling (o que verificar ao apresentar)

- **PrÃ©-flight (fail-fast)**: arquivos existem, tamanhos coerentes, gzip/tar Ã­ntegros, **CSVs vÃ¡lidos** do A/B encontrados.  
- **Profiling (pÃ³s-leitura)**:  
  - `orders/consumers/restaurants/abmap`: **schema** e **amostras**;  
  - faixa de datas (min/max) e **nulos em campos-chave**;  
  - distribuiÃ§Ã£o de **grupo** (controle vs tratamento) no A/B.

Esses passos mostram maturidade de engenharia e evitam â€œrodar com tabelas vaziasâ€.

---

## ğŸ“¦ SaÃ­das do ETL (em memÃ³ria)

- `orders_silver`: fato por pedido (UTC/BRT, valores, flags, atributos do consumidor e do restaurante, `is_target`).  
- `users_silver`: R/F/M por usuÃ¡rio + `is_target`, com `recency` calculado a partir do Ãºltimo `event_ts_utc`.

---

## ğŸ—‚ï¸ Resumo dos MÃ³dulos

### `src/etl.py`
FunÃ§Ãµes de:
- IngestÃ£o de dados brutos (JSON, CSV)
- Limpeza e conformidade de dados
- Joins e agregaÃ§Ãµes
- NormalizaÃ§Ã£o de timestamps

### `src/utils.py`
UtilitÃ¡rios para:
- ConfiguraÃ§Ã£o de SparkSession
- Carregamento de configuraÃ§Ãµes (YAML)
- Controle de seeds e benchmarking para shuffle partitions

### `src/checks.py`
FunÃ§Ãµes de validaÃ§Ã£o e prÃ©-checagem:
- ValidaÃ§Ã£o de arquivos gzip e tar
- Listagem de CSVs vÃ¡lidos para testes A/B
- Checagem de formatos de arquivos de pedidos

### `src/analysis_ab.py`
FunÃ§Ãµes de:
- MÃ©tricas A/B por grupo (Spark)
- Coleta de dados por usuÃ¡rio para testes (Pandas)
- Testes estatÃ­sticos (Welch t-test e z-test)
- Viabilidade financeira (ROI com premissas)

## Etapa 1 â€” AnÃ¡lise A/B de Cupons

**Objetivo**: medir impacto da campanha de cupons e avaliar viabilidade financeira.

### MÃ©tricas
- **GMV/usuÃ¡rio** (`gmv_user`)
- **Pedidos/usuÃ¡rio** (`pedidos_user`)
- **ConversÃ£o** (`conversao`): % de usuÃ¡rios com â‰¥1 pedido
- **AOV** (`aov`): ticket mÃ©dio por usuÃ¡rio (apenas usuÃ¡rios com pedidos)

### Testes
- **Welch t-test** para mÃ©dias (GMV/usuÃ¡rio, Pedidos/usuÃ¡rio, AOV)
- **Z-test** para proporÃ§Ãµes (ConversÃ£o)

### Viabilidade Financeira
- Receita incremental = *uplift_gmv_user* Ã— N_tratados Ã— *take_rate*
- Custo = N_tratados Ã— *redemption_rate* Ã— *coupon_cost*
- ROI = Receita incremental âˆ’ Custo

> ParÃ¢metros em `config/settings.yaml`:
> - `finance.take_rate` (padrÃ£o: 0.23)
> - `finance.coupon_cost_default` (padrÃ£o: 10.0)

## ğŸ“ˆ A/B, ROI e SegmentaÃ§Ã£o (no notebook)

- **A/B**: mÃ©tricas por usuÃ¡rio (**GMV/U, Pedidos/U, ConversÃ£o, AOV**), **Welch t-test** (mÃ©dias) e **z-test** (proporÃ§Ãµes). Opcional: **CUPED**.  
- **Viabilidade**: **ROI** e **sensibilidade** (take rate, custo do cupom, cobertura). **Premissas** ficam explÃ­citas no topo da seÃ§Ã£o.  
- **SegmentaÃ§Ã£o (RFM)**: regras claras e leitura do **uplift por segmento**, com **aÃ§Ãµes sugeridas** por pÃºblico.
- **Melhorias futuras**: **K-Means** como refinamento da segmentaÃ§Ã£o; **guardrails** adicionais para o novo A/B.

---

## ğŸ”’ Privacidade

- Dados PII **nÃ£o** sÃ£o mantidos nas camadas analÃ­ticas (hash/removidos).  
- Os arquivos de dados **nÃ£o** sÃ£o versionados no Git; sempre baixados de fontes configuradas em `settings.yaml`.

---

## ğŸ“Œ Resumo para a apresentaÃ§Ã£o

- **Por que Colab-only?** Reprodutibilidade e simplicidade para os avaliadores.  
- **Gargalo conhecido:** `orders` Ã© grande e gzip nÃ£o Ã© splittable â†’ leitura 1 task; depois **repartition + broadcast**.  
- **Qualidade:** prÃ©-flight fail-fast + profiling guiando o ETL; timezone/PII/validaÃ§Ãµes.  
- **A/B â†’ ROI â†’ RFM** na ordem pedida, com **premissas explÃ­citas** e **prÃ³ximos passos**.

---
