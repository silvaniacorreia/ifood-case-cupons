# Case iFood: Teste A/B Estrat√©gia de Cupons

Reposit√≥rio do case para **Analista de Dados** no iFood. Objetivo: analisar um **teste A/B** de cupons com foco em **reten√ß√£o** e crescimento.

> Execu√ß√£o **100% no Google Colab** para m√°xima reprodutibilidade (sem depend√™ncias locais).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

---

## üß≠ Escopo da an√°lise

1) **A/B de cupons**  
   (a) Definir **m√©tricas de sucesso** e checar **signific√¢ncia** no per√≠odo;  
   (b) **Viabilidade financeira** (ROI/payback) com **premissas expl√≠citas**;  
   (c) Recomendar **melhorias** e desenhar um **novo A/B** (m√©tricas/guardrails).

2) **Segmenta√ß√£o de usu√°rios**  
   (a) **RFM** como baseline (crit√©rios e racional claros);  
   (b) Ler o **efeito do A/B por segmento** e propor **a√ß√µes por p√∫blico**.

3) **Pr√≥ximos passos**  
   Estimativa de **impacto** (financeiro ou n√£o) e sugest√µes de **processo/teste**.

---

## üóÇÔ∏è Estrutura do reposit√≥rio

```
ifood-case-cupons/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ pipeline_analise_completa.ipynb # notebook principal (orquestra√ß√£o da an√°lise)
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ utils.py                       # settings + spark + seeds
‚îÇ  ‚îú‚îÄ etl.py                         # ingest√£o + limpeza + joins + silvers
‚îÇ  ‚îî‚îÄ analysis_ab.py                 # fun√ß√µes de an√°lise A/B
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
‚îú‚îÄ config/
‚îÇ  ‚îî‚îÄ settings.yaml                  # fontes + par√¢metros (ver abaixo)
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                           # arquivos baixados
‚îÇ  ‚îî‚îÄ processed/                     # parquet gerados pelo ETL
‚îî‚îÄ report/                           # relat√≥rio final (PDF)
```

---

## üß± Arquitetura & otimiza√ß√µes

### ETL otimizado
1. **Janela de an√°lise antes do join (robusta a outliers)**  
   A janela √© **inferida automaticamente pelos quantis 1‚Äì99% de `event_ts_utc`** e aplicada **antes** dos joins.  
   **Impacto:** reduz shuffle, mem√≥ria e custo de join sem ser ‚Äúpuxada‚Äù por timestamps an√¥malos.

2. **Proje√ß√£o m√≠nima de colunas**  
   Selecionamos apenas as colunas necess√°rias antes dos joins.  
   **Impacto:** menos shuffle/mem√≥ria, joins e writes mais r√°pidos.

3. **Reparticionamento por chave de join**  
   `orders` √© reparticionado por `customer_id` usando `spark.sql.shuffle.partitions`.  
   **Impacto:** melhor balanceamento no shuffle durante o join.

4. **Spark-first & amostragem**
   ‚Ä¢ M√©tricas descritivas e robustas (medianas, p95, taxa de heavy users) s√£o calculadas no Spark e s√≥ ent√£o convertidas para pandas quando necess√°rio (tabelas pequenas).
   ‚Ä¢ Amostragem opcional para testes/plots: `collect_user_level_for_tests(sample_frac=...)` amostra no Spark antes do `toPandas()`, reduzindo tempo e mem√≥ria (use 0.20‚Äì0.30 como refer√™ncia).
   ‚Ä¢ Arrow ativado quando dispon√≠vel, acelerando `toPandas()` de agregados pequenos. (Habilitado via `spark.sql.execution.arrow.pyspark.enabled=true`).
   ‚Ä¢ Reuso de shards do `order.json.gz`: `scripts/download_data.py` grava `data/raw/orders_sharded/` e o ETL l√™ diretamente desses shards em execu√ß√µes seguintes.

4. **Broadcast de dimens√£o pequena**  
   `restaurants` √© broadcast para habilitar broadcast-hash join.  
   **Impacto:** elimina shuffle dessa dimens√£o e acelera o join.

5. **Controles de verbosidade e cache**  
   `verbose=False` por padr√£o (evita `count()`/`show()` desnecess√°rios) e `cache_intermediates=False` (reduz risco de OOM).

6. **Escrita em Parquet opcional e leve**  
   Persist√™ncia √© **opcional** (desligada por padr√£o). Ao salvar, usamos `partitionBy(event_date_brt)` e **coalesce** para evitar explos√£o de arquivos.

### Configura√ß√µes do Spark para Colab
- **AQE ligado:** `spark.sql.adaptive.enabled=true`
- **Serializer Kryo:** menor overhead de serializa√ß√£o
- **Timezone fixa (UTC):** convers√µes consistentes
- **`spark.sql.shuffle.partitions`:** 32 (definido por testes de benchmark) via `config/settings.yaml`

**Exemplo de `runtime.spark.conf` no Colab:**
```yaml
runtime:
  spark:
    app_name: "ifood-case-cupons"
    driver_memory: "12g"
    shuffle_partitions: 128
    conf:
      spark.master: "local[*]"
      spark.sql.adaptive.enabled: "true"
      spark.sql.adaptive.coalescePartitions.enabled: "true"
      spark.sql.files.maxPartitionBytes: "64m"
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"
      spark.memory.fraction: "0.6"
      spark.sql.autoBroadcastJoinThreshold: "50MB"
```

### Leituras e conformiza√ß√µes robustas
- **Reader resiliente de orders:** detecta NDJSON ou JSON array e tem fallback com gzip/json.
- **Sanitiza√ß√£o de PII e tipagem:** normalizamos timestamps, lat/long, flags, calculamos `basket_size` de forma segura, e removemos/hasheamos PII.
- **Checagens pr√©vias (preflight):** verificamos tamanhos, compress√£o e candidatos do `ab_test_ref` antes de acionar o Spark; falha cedo em caso de problema.

### Enriquecimentos para an√°lise (camada ‚Äúsilver‚Äù)
- **`origin_platform` nulo ‚Üí `"unknown"`** (evita perdas em cortes por canal).
- **Campos de consumidor faltantes**: vers√µes limpas para segmenta√ß√£o (ex.: `language_clean = coalesce(language, 'unknown')`), mantendo os originais para auditoria.
- **Atributos de restaurante imputados (colunas paralelas)**  
  - `minimum_order_value_imputed`: mediana por (`merchant_city`, `price_range`) com fallback por `price_range`.  
  - `delivery_time_imputed`: mediana por `price_range`.  
  As colunas **originais s√£o preservadas**; as vers√µes imputadas s√£o usadas apenas para diagn√≥stico/controle (ex.: balance check/CUPED).

---

## ‚ñ∂Ô∏è Como executar

### Execu√ß√£o no Colab

1. Abra o notebook **no Colab**:  
   [**pipeline_analise_completa.ipynb**](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

2. **Runtime ‚Üí Run all**. A primeira c√©lula:
   - clona/atualiza o reposit√≥rio;
   - instala as depend√™ncias de `requirements.txt`;
   - roda o **download program√°tico** (`scripts/download_data.py`);

3. O notebook ent√£o executa:
   - **Pr√©-flight** (fail-fast) dos arquivos baixados;  
   - **Profiling** dos 4 dataframes brutos;  
   - **ETL** completo com normaliza√ß√£o de timezone/PII e joins;  
   - **A/B**, **viabilidade** e **RFM** (em sequ√™ncia).

> **Tempo de execu√ß√£o:** a leitura de `orders` (~1.6 GB gz) pode levar alguns minutos no Colab (gzip n√£o √© splittable). Depois da leitura, o ETL **reparticiona por `customer_id`** para paralelizar os joins.

---

## ‚öôÔ∏è Configura√ß√µes importantes (`config/settings.yaml`)

| Caminho                         | Descri√ß√£o |
|--------------------------------|-----------|
| `data.raw_dir`                 | Pasta dos brutos (default: `data/raw`) |
| `data.processed_dir`           | Pasta dos parquet (se habilitar salvar) |
| `runtime.spark.driver_memory`  | Mem√≥ria do driver no Colab (`12g`) |
| `analysis.business_tz`         | TZ de neg√≥cio (default `America/Sao_Paulo`) |
| `analysis.experiment_window`   | **auto-inferida** |
| `analysis.auto_infer_window`   | `true`/`false` ‚Äî ativa a infer√™ncia de janela |
| `analysis.treat_is_target_null_as_control` | `false` por padr√£o (linhas sem grupo s√£o exclu√≠das) |
| `analysis.winsorize`/`use_cuped` | Par√¢metros para A/B (aplicados nas an√°lises) |
| `runtime.spark.conf.*`        | Confs avan√ßadas do Spark (AQE, Kryo, parti√ß√µes, broadcast etc.) |

---

## üß± Decis√µes t√©cnicas & otimiza√ß√µes de desempenho

### Formato e leitura dos dados
- **Formato de `orders`**: detectado como **NDJSON**; leitura com `spark.read.json(...)`.  
  - Como o arquivo √© grande e gzip n√£o √© splittable, a leitura inicial roda em 1 task; ap√≥s ler, fazemos:  
    **`o = o.repartition(spark.sql.shuffle.partitions, 'customer_id')`** para **distribuir** o trabalho nos joins.
  - **Broadcast** de dimens√µes: `restaurants` sempre (pequena) e `abmap` se `count ‚â§ 2M`.

### Configura√ß√£o do Spark
- **driver_memory**: ajustado para **12g** para evitar problemas de mem√≥ria.

### Persist√™ncia e formato de sa√≠da
- **Persist√™ncia estrat√©gica**: usamos `.cache()` para evitar recomputa√ß√£o em etapas subsequentes.  
- **Parquet**: ap√≥s o ETL, os DataFrames processados podem ser salvos em Parquet para acelerar leituras futuras.

---

## üß∞ O notebook como orquestrador t√©cnico

O notebook **pipeline_analise_completa.ipynb** funciona como um **orquestrador t√©cnico** das tarefas de an√°lise, integrando os diferentes m√≥dulos do reposit√≥rio:

1. **Configura√ß√£o inicial**:
   - Clona o reposit√≥rio e instala as depend√™ncias.
   - Faz o download program√°tico dos dados brutos.

2. **Configura√ß√£o do Spark**:
   - L√™ as configura√ß√µes do arquivo `settings.yaml` e inicializa o Spark com par√¢metros ajustados para o Colab.

3. **Execu√ß√£o das tarefas**:
   - **Pr√©-flight**: valida√ß√£o dos arquivos brutos.
   - **ETL**: utiliza fun√ß√µes do m√≥dulo `src/etl.py` para ingest√£o, conforma√ß√£o e gera√ß√£o dos DataFrames "silver".
   - **An√°lises**: executa m√©tricas de A/B, ROI e segmenta√ß√£o, utilizando fun√ß√µes espec√≠ficas dos m√≥dulos `src/utils.py` e `src/checks.py`.

4. **Orquestra√ß√£o**:
   - O notebook organiza a execu√ß√£o das etapas de forma sequencial, garantindo que cada tarefa seja realizada com base nos resultados da anterior.

---

## ‚úÖ Pr√©-flight & Profiling (o que verificar ao apresentar)

- **Pr√©-flight (fail-fast)**: arquivos existem, tamanhos coerentes, gzip/tar √≠ntegros, **CSVs v√°lidos** do A/B encontrados.  
- **Profiling (p√≥s-leitura)**:  
  - `orders/consumers/restaurants/abmap`: **schema** e **amostras**;  
  - faixa de datas (min/max) e **nulos em campos-chave**;  
  - distribui√ß√£o de **grupo** (controle vs tratamento) no A/B.

Esses passos mostram maturidade de engenharia e evitam ‚Äúrodar com tabelas vazias‚Äù.

---

## üì¶ Sa√≠das do ETL (em mem√≥ria)

- `orders_silver`: fato por pedido (UTC/BRT, valores, flags, atributos do consumidor e do restaurante, `is_target`).  
- `users_silver`: R/F/M por usu√°rio + `is_target`, com `recency` calculado a partir do √∫ltimo `event_ts_utc`.

---

## üóÇÔ∏è Resumo dos M√≥dulos

### `src/etl.py`
Fun√ß√µes de:
- Ingest√£o de dados brutos (JSON, CSV)
- Limpeza e conformidade de dados
- Joins e agrega√ß√µes
- Normaliza√ß√£o de timestamps

### `src/utils.py`
Utilit√°rios para:
- Configura√ß√£o de SparkSession
- Carregamento de configura√ß√µes (YAML)
- Controle de seeds e benchmarking para shuffle

### `src/checks.py`
Fun√ß√µes de valida√ß√£o e pr√©-checagem:
- Valida√ß√£o de arquivos gzip e tar
- Listagem de CSVs v√°lidos para testes A/B
- Checagem de formatos de arquivos de pedidos

### `src/analysis_ab.py`
Fun√ß√µes de:
- M√©tricas A/B por grupo (Spark) e robustas (Spark, `percentile_approx`); coleta para testes em pandas com amostragem opcional.

## Etapas da An√°lise

### 1. Prepara√ß√£o e Limpeza de Dados

* Leitura de shards e normaliza√ß√£o de schemas.
* Tratamento de nulos e imputa√ß√£o:

  * `minimum_order_value_imputed`: mediana por `price_range`.
  * `delivery_time_imputed`: mediana por `price_range`.
* Garantia de unicidade de `order_id` (checagem de duplicatas com colunas principais).

### 2. Checagens P√≥s-ETL

* Faixa de datas (UTC) de `orders_silver`.
* Distribui√ß√£o A/B (`users_silver`).
* Contagem de nulos em colunas-chave.
* Amostragem de previews para sanity check.
* Checagem adicional de duplicatas por conte√∫do de ordem (mesmo cliente, valor, data).

### 3. An√°lise A/B (Tarefa 1)

#### M√©tricas de impacto

* **GMV/usu√°rio (m√©dia)**
* **Pedidos/usu√°rio (m√©dia)**
* **Convers√£o (propor√ß√£o de usu√°rios com ‚â•1 pedido)**
* **AOV (m√©dia do valor dos pedidos)**

Al√©m das m√©dias, reportamos tamb√©m:

* **Mediana de GMV/usu√°rio, Pedidos/usu√°rio e AOV** (reduz outliers)
* **p95 de GMV/usu√°rio, Pedidos/usu√°rio e AOV** (captura a cauda superior sem extremos)
* **Heavy users (% com ‚â•3 pedidos no per√≠odo)**

Tamb√©m reportamos m√©tricas robustas (medianas, p95, heavy users) calculadas no Spark e usamos amostragem para gr√°ficos e testes n√£o-param√©tricos quando necess√°rio.

#### Testes estat√≠sticos

* **Welch t-test** (m√©dias, explorat√≥rio)  
* **Mann‚ÄìWhitney U** (robusto, apresentado no relat√≥rio)  
* **Z-test de propor√ß√µes** (para convers√£o; n√£o aplic√°vel no experimento atual)

#### Premissas financeiras

As premissas padr√£o (take rate, custo do cupom) s√£o lidas de `config/settings.yaml` e podem ser sobrescritas no notebook.
Exemplo atual: `take_rate=0.23`, `coupon_cost=10.0`.
Outras premissas (taxa de resgate, horizonte temporal) tamb√©m podem ser parametrizadas no notebook ao chamar `financial_viability`.

#### Indicadores financeiros

Calculados na fun√ß√£o `financial_viability`, a partir de premissas expl√≠citas e com agregass√£o no Spark:

* **ROI absoluto e por usu√°rio**
* **CAC (Custo de Aquisi√ß√£o de Cliente)**
* **LTV (Lifetime Value, horizonte do experimento)**
* **LTV:CAC (sustentabilidade financeira)**

#### Relat√≥rio final (executivo)

No relat√≥rio, mantemos apenas:

* M√©tricas robustas (medianas, p95, heavy users)
* Testes robustos (Mann‚ÄìWhitney U)
* ROI, LTV, CAC, LTV:CAC  
* Premissas financeiras claras (take rate, valor do cupom, taxa de resgate)

### 4. Segmenta√ß√£o de usu√°rios (Tarefa 2)

#### Objetivo
Agrupar clientes com comportamentos semelhantes para **direcionar o cupom certo ao p√∫blico certo**, maximizando engajamento/reten√ß√£o e **ROI**.

#### (a) Crit√©rios de segmenta√ß√£o e racional
- **Frequ√™ncia (Heavy user)** ‚Äî *heavy* (‚â• 3 pedidos no per√≠odo) vs *n√£o-heavy* (< 3).  
  *Por qu√™?* frequ√™ncia √© o melhor preditor de valor; separa quem j√° tem h√°bito de quem ainda est√° ‚Äúem forma√ß√£o‚Äù.
- **Plataforma de origem** ‚Äî `android`, `ios`, `desktop`.  
  *Por qu√™?* jornada/ticket variam por dispositivo/canal.
- **RFM (Recency-Frequency-Monetary)** ‚Äî c√≥digos `111‚Äì555` (1=baixo, 5=alto).  
  *Por qu√™?* permite graduar incentivo por valor e rec√™ncia.
- **Novo vs recorrente** ‚Äî sinal informativo; nesta base, ‚Äúnovo‚Äù √© residual (amostra focada em quem j√° comprou).

> **Como ler os gr√°ficos:** mostramos o **valor t√≠pico por cliente (mediana)**; **p95** ilustra a ‚Äúponta de cima‚Äù; **heavy users** √© a % de clientes com ‚â•3 pedidos.  
> **Financeiro (R$)** √© calculado sobre **100% da base** no Spark (agregados), e s√≥ o **resumo** vem para o relat√≥rio.

**Artefatos gerados (salvos em `outputs/`):**
- Tabelas robustas (medianas/p95): `robust_heavy_user.csv`, `robust_origin_platform.csv`, `robust_rfm_segment.csv`, `robust_is_new_customer.csv`.
- Tabelas de m√©dias (sanity check / ap√™ndice): `ab_*_summary.csv`.
- Figuras: `outputs/figs_segments/` (barras de **medianas**, p95 e taxa de heavy; boxplots/hist EDA).

## üîí Privacidade

- Dados PII **n√£o** s√£o mantidos nas camadas anal√≠ticas (hash/removidos).  
- Os arquivos de dados **n√£o** s√£o versionados no Git; sempre baixados de fontes configuradas em `settings.yaml`.


