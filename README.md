# Case iFood: Teste A/B Estrat√©gia de Cupons

Reposit√≥rio do case para **Analista de Dados** no iFood. Objetivo: analisar um **teste A/B** de cupons com foco em **reten√ß√£o** e crescimento.

> Execu√ß√£o **100% no Google Colab** para m√°xima reprodutibilidade (sem depend√™ncias locais).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/analise_completa.ipynb)

---

## üß≠ Escopo da an√°lise

1) **A/B de cupons**  
   (a) Definir **m√©tricas de sucesso** e checar **signific√¢ncia** no per√≠odo;  
   (b) **Viabilidade financeira** (ROI/payback) com **premissas expl√≠citas**;  
   (c) Recomendar **melhorias** e desenhar um **novo A/B** (m√©tricas/guardrails).

2) **Segmenta√ß√£o de usu√°rios**  
   (a) **RFM** como baseline (crit√©rios e racional claros);  
   (b) Ler o **efeito do A/B por segmento** e propor **a√ß√µes por p√∫blico**.

3) **Pr√≥ximos passos**  
   Estimativa de **impacto** (financeiro ou n√£o) e sugest√µes de **processo/teste**.

---

## üóÇÔ∏è Estrutura do reposit√≥rio

```
ifood-case-cupons/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ pipeline_analise_completa.ipynb  # notebook principal (orquestra√ß√£o da an√°lise)
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ utils.py                       # settings + spark + seeds
‚îÇ  ‚îî‚îÄ etl.py                         # ingest√£o + limpeza + joins + silvers
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
‚îú‚îÄ config/
‚îÇ  ‚îî‚îÄ settings.yaml                  # fontes + par√¢metros (ver abaixo)
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                           # arquivos baixados
‚îÇ  ‚îî‚îÄ processed/                     # parquet gerados pelo ETL
‚îî‚îÄ report/                           # relat√≥rio final (PDF)
```

---

## ‚ñ∂Ô∏è Como executar

### Execu√ß√£o no Colab

1. Abra o notebook **no Colab**:  
   [**analise_completa.ipynb**](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/analise_completa.ipynb)

2. Menu **Ambiente de Execu√ß√£o ‚Üí Executar Tudo**. A primeira c√©lula:
   - clona/atualiza o reposit√≥rio;  
   - instala as depend√™ncias do `requirements.txt`;  
   - roda o **download program√°tico** (`scripts/download_data.py`).

3. O notebook ent√£o executa:
   - **Pr√©-flight** (fail-fast) dos arquivos baixados;  
   - **Profiling** dos 4 dataframes brutos;  
   - **ETL** completo com normaliza√ß√£o de timezone/PII e joins;  
   - **A/B**, **viabilidade** e **RFM** (em sequ√™ncia).

> **Tempo de execu√ß√£o:** a leitura de `orders` (~1.6 GB gz) pode levar alguns minutos no Colab (gzip n√£o √© splittable). Depois da leitura, o ETL **reparticiona por `customer_id`** para paralelizar os joins.

---

## ‚öôÔ∏è Configura√ß√µes importantes (`config/settings.yaml`)

| Caminho                         | Descri√ß√£o |
|--------------------------------|-----------|
| `data.raw_dir`                 | Pasta dos brutos (default: `data/raw`) |
| `data.processed_dir`           | Pasta dos parquet (se habilitar salvar) |
| `runtime.spark.shuffle_partitions` | N¬∫ de parti√ß√µes p/ shuffles/joins (usado no `repartition`) |
| `runtime.spark.driver_memory`  | Mem√≥ria do driver no Colab (`12g`) |
| `analysis.business_tz`         | TZ de neg√≥cio (default `America/Sao_Paulo`) |
| `analysis.experiment_window`   | **auto-inferida** |
| `analysis.auto_infer_window`   | `true`/`false` ‚Äî ativa a infer√™ncia de janela |
| `analysis.treat_is_target_null_as_control` | `false` por padr√£o (linhas sem grupo s√£o exclu√≠das) |
| `analysis.winsorize`/`use_cuped` | Par√¢metros para A/B (aplicados nas an√°lises) |

---

## üß± Decis√µes t√©cnicas & otimiza√ß√µes de desempenho

### Formato e leitura dos dados
- **Formato de `orders`**: detectado como **NDJSON**; leitura com `spark.read.json(...)`.  
  - Como o arquivo √© grande e gzip n√£o √© splittable, a leitura inicial roda em 1 task; ap√≥s ler, fazemos:  
    **`o = o.repartition(spark.sql.shuffle.partitions, 'customer_id')`** para **distribuir** o trabalho nos joins.
  - **Broadcast** de dimens√µes: `restaurants` sempre (pequena) e `abmap` se `count ‚â§ 2M`.

### Configura√ß√£o do Spark
- **shuffle_partitions**: otimizado para **128** com base em benchmarks no Colab.  
- **driver_memory**: ajustado para **12g** para evitar problemas de mem√≥ria.

### Persist√™ncia e formato de sa√≠da
- **Persist√™ncia estrat√©gica**: usamos `.cache()` para evitar recomputa√ß√£o em etapas subsequentes.  
- **Parquet**: ap√≥s o ETL, os DataFrames processados podem ser salvos em Parquet para acelerar leituras futuras.

---

## üß∞ O notebook como orquestrador t√©cnico

O notebook **pipeline_analise_completa.ipynb** funciona como um **orquestrador t√©cnico** das tarefas de an√°lise, integrando os diferentes m√≥dulos do reposit√≥rio:

1. **Configura√ß√£o inicial**:
   - Clona o reposit√≥rio e instala as depend√™ncias.
   - Faz o download program√°tico dos dados brutos.

2. **Configura√ß√£o do Spark**:
   - L√™ as configura√ß√µes do arquivo `settings.yaml` e inicializa o Spark com par√¢metros ajustados para o Colab.

3. **Execu√ß√£o das tarefas**:
   - **Pr√©-flight**: valida√ß√£o dos arquivos brutos.
   - **ETL**: utiliza fun√ß√µes do m√≥dulo `src/etl.py` para ingest√£o, conforma√ß√£o e gera√ß√£o dos DataFrames "silver".
   - **An√°lises**: executa m√©tricas de A/B, ROI e segmenta√ß√£o, utilizando fun√ß√µes espec√≠ficas dos m√≥dulos `src/utils.py` e `src/checks.py`.

4. **Orquestra√ß√£o**:
   - O notebook organiza a execu√ß√£o das etapas de forma sequencial, garantindo que cada tarefa seja realizada com base nos resultados da anterior.

---

## ‚úÖ Pr√©-flight & Profiling (o que verificar ao apresentar)

- **Pr√©-flight (fail-fast)**: arquivos existem, tamanhos coerentes, gzip/tar √≠ntegros, **CSVs v√°lidos** do A/B encontrados.  
- **Profiling (p√≥s-leitura)**:  
  - `orders/consumers/restaurants/abmap`: **schema** e **amostras**;  
  - faixa de datas (min/max) e **nulos em campos-chave**;  
  - distribui√ß√£o de **grupo** (controle vs tratamento) no A/B.

Esses passos mostram maturidade de engenharia e evitam ‚Äúrodar com tabelas vazias‚Äù.

---

## üì¶ Sa√≠das do ETL (em mem√≥ria)

- `orders_silver`: fato por pedido (UTC/BRT, valores, flags, atributos do consumidor e do restaurante, `is_target`).  
- `users_silver`: R/F/M por usu√°rio + `is_target`, com `recency` calculado a partir do √∫ltimo `event_ts_utc`.

---

## üìà A/B, ROI e Segmenta√ß√£o (no notebook)

- **A/B**: m√©tricas por usu√°rio (**GMV/U, Pedidos/U, Convers√£o, AOV**), **Welch t-test** (m√©dias) e **z-test** (propor√ß√µes). Opcional: **CUPED**.  
- **Viabilidade**: **ROI** e **sensibilidade** (take rate, custo do cupom, cobertura). **Premissas** ficam expl√≠citas no topo da se√ß√£o.  
- **Segmenta√ß√£o (RFM)**: regras claras e leitura do **uplift por segmento**, com **a√ß√µes sugeridas** por p√∫blico.
- **Melhorias futuras**: **K-Means** como refinamento da segmenta√ß√£o; **guardrails** adicionais para o novo A/B.

---

## üß∞ Solu√ß√£o de problemas (Colab)

- **Execu√ß√£o lenta no come√ßo**: esperado por causa de `orders` (~1.6 GB gz, gzip n√£o splittable). Ap√≥s a leitura, o ETL paraleliza.  
- **Out of memory**: aumente `runtime.spark.driver_memory` no `settings.yaml` (ex.: `12g`) e reduza o n√∫mero de colunas exibidas (menos `.toPandas()` em previews).  
- **Erro de rede no download**: reexecute a 1¬™ c√©lula (o script √© idempotente).  
- **CSV do A/B n√£o encontrado**: o pr√©-flight falha cedo e indica o problema na pasta `data/raw/ab_test_ref_extracted/`.

---

## üîí Privacidade

- Dados PII **n√£o** s√£o mantidos nas camadas anal√≠ticas (hash/removidos).  
- Os arquivos de dados **n√£o** s√£o versionados no Git; sempre baixados de fontes configuradas em `settings.yaml`.

---

## üìå Resumo para a apresenta√ß√£o

- **Por que Colab-only?** Reprodutibilidade e simplicidade para os avaliadores.  
- **Gargalo conhecido**: `orders` √© grande e gzip n√£o √© splittable ‚Üí leitura 1 task; depois **repartition + broadcast**.  
- **Qualidade**: pr√©-flight fail-fast + profiling guiando o ETL; timezone/PII/valida√ß√µes.  
- **A/B ‚Üí ROI ‚Üí RFM** na ordem pedida, com **premissas expl√≠citas** e **pr√≥ximos passos**.

---
