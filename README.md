# iFood Case ‚Äì Cupons

Reposit√≥rio para o case de **Analista de Dados** do iFood. Objetivo: analisar um **teste A/B** de uma estrat√©gia de **cupons** com foco em **reten√ß√£o** e crescimento, seguindo a ordem e escopo pedidos no case.

---

## ‚ú® Escopo do Case (ordem de entrega)

1) **Teste A/B (Campanha de Cupons)**
   - **(a)** Definir **m√©tricas de sucesso** e analisar **impacto estatisticamente significativo** no per√≠odo avaliado.
   - **(b)** Realizar **an√°lise de viabilidade financeira** (ex.: ROI / payback), deixando **premissas expl√≠citas**.
   - **(c)** Recomendar **oportunidades de melhoria** e **desenhar um novo A/B** (desenho experimental, m√©tricas e guardrails).

2) **Segmenta√ß√£o de Usu√°rios**
   - **(a)** Estabelecer **crit√©rios/regras** dos segmentos (neste projeto: **RFM** como baseline), explicando o racional.
   - **(b)** Analisar **resultados do A/B por segmento** e **propor a√ß√µes** espec√≠ficas.

3) **Recomenda√ß√µes e Pr√≥ximos Passos**
   - Plano com **previs√£o de impacto** (financeiro ou n√£o) para as **Lideran√ßas de Neg√≥cio**.
   - **Melhorias de processo/teste** e **estrat√©gias por segmento**.
   - **Premissas documentadas** quando necess√°rio.

---

## üöÄ Status (at√© agora)

- **Bootstrap reprodut√≠vel** (local e Colab) no `00_setup_and_checks.ipynb`:
  - Colab: clona o repo, instala deps, baixa e prepara os dados (**download program√°tico**).
  - Local: garante paths e roda o mesmo script de download.
- **Download program√°tico** + extra√ß√£o: `scripts/download_data.py`
  - Baixa os 4 insumos de `config/settings.yaml`.
  - **Extrai** apenas o `ab_test_ref.tar.gz` (o Spark l√™ `.gz` diretamente).
  - **Ignora artefatos** do macOS `._*.csv` e `.DS_Store` ao localizar o CSV do A/B.
- **ETL base** (`01_etl_pyspark.ipynb` + `src/etl.py`):
  - **Orders**: tipagem, normaliza√ß√£o de **timezone** (UTC can√¥nico; BRT para ‚Äúdia de neg√≥cio‚Äù), remo√ß√£o/hasheamento de **PII**, valida√ß√µes (duplicados, valores, geolocaliza√ß√£o), escolha do **timestamp do evento** (agendado vs criado).
  - **Consumers / Restaurants**: tipagem, normaliza√ß√£o, remo√ß√£o/hasheamento de PII e renomea√ß√µes para joins seguros.
  - **Janela do experimento**: parametrizada via `settings.yaml` **ou** **inferida automaticamente** dos dados quando n√£o definida.
  - Sa√≠das: `data/processed/orders_silver.parquet` (fato por pedido) e `data/processed/users_silver.parquet` (R/F/M por usu√°rio).
- **Checks de qualidade** no final do notebook (nulos, unicidade, janelas, split A/B, distribui√ß√µes).

> Pr√≥ximas etapas (em constru√ß√£o): `02_abtest_core.ipynb` (m√©tricas + testes), `03_financial_viability.ipynb` (ROI) e `04_segments_ab_by_segment.ipynb` (RFM + A/B por segmento).

---

## üìÅ Estrutura do reposit√≥rio

```
ifood-case-cupons/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ 00_setup_and_checks.ipynb      # bootstrap + download program√°tico + smoke Spark
‚îÇ  ‚îú‚îÄ 01_etl_pyspark.ipynb           # ETL e silvers (orders/users)
‚îÇ  ‚îú‚îÄ 02_abtest_core.ipynb           # (em constru√ß√£o) A/B m√©tricas + signific√¢ncia
‚îÇ  ‚îú‚îÄ 03_financial_viability.ipynb   # (em constru√ß√£o) ROI e sensibilidade
‚îÇ  ‚îî‚îÄ 04_segments_ab_by_segment.ipynb# (em constru√ß√£o) RFM + leitura do A/B por segmento
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ utils.py                       # settings + spark + seeds
‚îÇ  ‚îî‚îÄ etl.py                         # ingest√£o + limpeza + joins + silvers
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
‚îú‚îÄ config/
‚îÇ  ‚îú‚îÄ settings.example.yaml
‚îÇ  ‚îî‚îÄ settings.yaml                  # fontes + par√¢metros (ver abaixo)
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                           # arquivos baixados
‚îÇ  ‚îî‚îÄ processed/                     # parquet gerados pelo ETL
‚îî‚îÄ report/                           # relat√≥rio final (PDF)
```

---

## üîó Acesso r√°pido aos Notebooks (clique com bot√£o direito e abra em nova aba)

- **Setup & Checks**  
  <a href="https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/00_setup_and_checks.ipynb" target="_blank" rel="noopener">Abrir no Colab</a> ¬∑
  <a href="https://github.com/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/00_setup_and_checks.ipynb" target="_blank" rel="noopener">Ver no GitHub</a>

- **ETL (PySpark)**  
  <a href="https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/01_etl_pyspark.ipynb" target="_blank" rel="noopener">Abrir no Colab</a> ¬∑
  <a href="https://github.com/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/01_etl_pyspark.ipynb" target="_blank" rel="noopener">Ver no GitHub</a>

*(Os notebooks 02/03/04 aparecer√£o aqui quando forem versionados.)*

---

## Execu√ß√£o

### Execu√ß√£o no **Colab** (avaliadores)
1. Abra **cada notebook** pelos links acima (o Colab abre em nova aba).  
2. Em `00_setup_and_checks`: **Runtime ‚Üí Run all**. A primeira c√©lula clona o repo, instala deps e roda o **download program√°tico**; a segunda faz o **smoke** do Spark.  
3. Em `01_etl_pyspark`: **Run all** para gerar `orders_silver.parquet` e `users_silver.parquet` em `data/processed/`.


### Execu√ß√£o **local** (desenvolvedores)

#### Pr√©-requisitos
- **Python 3.10+**
- **JDK 11+** (Spark)
- **(Windows)**: [winutils.exe](https://github.com/cdarlint/winutils) compat√≠vel com sua vers√£o do Hadoop (veja abaixo)
- **(Recomendado para Windows):** [WSL 2 (Ubuntu)](https://learn.microsoft.com/pt-br/windows/wsl/install)

#### Ambiente local (Windows, macOS, Linux)
```bash
# 1) Ambiente virtual
python -m venv .venv && . .venv/Scripts/activate  # Windows PowerShell
# ou: source .venv/bin/activate                    # macOS/Linux/WSL
pip install -r requirements.txt
python -m ipykernel install --user --name ifood-case

# 2) Download program√°tico dos dados
python scripts/download_data.py

# 3) Jupyter
jupyter notebook  # ou jupyter lab
# Abra 00_setup_and_checks.ipynb (Run all) e depois 01_etl_pyspark.ipynb (Run all)
```

#### ‚ö†Ô∏è Observa√ß√µes importantes para Windows

- Para **escrever Parquet** com Spark, √© necess√°rio configurar o **winutils.exe**:
  1. Descubra a vers√£o do Hadoop usada pelo Spark:
     ```python
     print("Spark:", spark.version)
     hadoop_ver = spark.sparkContext._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()
     print("Hadoop:", hadoop_ver)
     ```
  2. Baixe o `winutils.exe` da **mesma linha de vers√£o** (ex: [winutils/hadoop-3.3.1](https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.1)) e coloque em `C:\hadoop\hadoop-<versao>\bin\winutils.exe`.
  3. Exporte as vari√°veis de ambiente antes de abrir o Jupyter:
     ```powershell
     $env:HADOOP_HOME="C:\hadoop\hadoop-3.3.4"
     $env:PATH="$env:HADOOP_HOME\bin;$env:PATH"
     ```
  4. Reinicie o terminal/Jupyter ap√≥s configurar.

- **Problemas comuns no Windows**:
  - Erros de permiss√£o ou `UnsatisfiedLinkError` ao salvar Parquet s√£o comuns. Se persistirem, **use WSL** (veja abaixo).

---

### Execu√ß√£o via **WSL (Windows Subsystem for Linux) ‚Äì Recomendado para Windows**

Rodar o projeto pelo WSL (Ubuntu) elimina problemas de compatibilidade do Spark/Hadoop no Windows.

#### 1. Instale o WSL e Ubuntu
No PowerShell (como administrador):
```powershell
wsl --install -d Ubuntu
```
Reinicie o computador se solicitado.

#### 2. Abra o Ubuntu (WSL) e prepare o ambiente
No Ubuntu (WSL):
```bash
# Instale depend√™ncias
sudo apt update
sudo apt install openjdk-11-jdk python3-pip python3-venv git

# Navegue at√© a pasta do projeto (acessando arquivos do Windows)
cd /mnt/c/Users/silva/OneDrive/Documentos/portf√≥lio/ifood-case-cupons

# OU (recomendado): copie o projeto para o Linux
cp -r /mnt/c/Users/silva/OneDrive/Documentos/portf√≥lio/ifood-case-cupons ~/
cd ~/ifood-case-cupons

# Crie e ative o ambiente virtual
python3 -m venv .venv
source .venv/bin/activate

# Instale as depend√™ncias
pip install -r requirements.txt

# Baixe os dados
python scripts/download_data.py

# Rode o Jupyter
jupyter notebook
```
Abra o link gerado no navegador do Windows.

#### 3. (Opcional) Use o VS Code com WSL
- Instale a extens√£o **Remote - WSL** no VS Code.
- Abra o projeto pelo comando:  
  `Remote-WSL: Open Folder in WSL`
- O terminal, Python e Jupyter rodar√£o no Linux, mas voc√™ edita normalmente pelo VS Code no Windows.

---

**Resumo das recomenda√ß√µes:**
- **Windows:** Prefira rodar pelo WSL para evitar problemas de Spark/Hadoop.
- **Ambiente virtual:** Sempre crie o venv pelo pr√≥prio sistema (n√£o misture venv do Windows com WSL).
- **Permiss√µes:** Se rodar em `/mnt/c/...`, pode haver problemas de permiss√£o. O ideal √© copiar o projeto para o Linux (`~/`).
- **Jupyter:** O link do Jupyter rodando no WSL pode ser aberto normalmente no navegador do Windows.

---

## üß± Premissas & Decis√µes de Projeto

- **Timezone**: timestamps normalizados para **UTC** (`event_ts_utc`); para relat√≥rios di√°rios, usamos **BRT** (`event_date_brt`).  
- **Evento do pedido**: se `order_scheduled==true` **e** `order_scheduled_date` existir, usamos a data agendada; caso contr√°rio, a data de cria√ß√£o do pedido.  
- **Janela do experimento**: se `analysis.experiment_window` estiver **ausente**, o ETL **infere** automaticamente `[min(data), max(data)+1d)` em UTC e aplica o filtro.  
- **A/B sem marca√ß√£o**: `treat_is_target_null_as_control=false` por padr√£o (linhas sem `is_target` s√£o exclu√≠das, para evitar vi√©s).  
- **PII**: `cpf` e telefone s√£o **hasheados**; nome e endere√ßo s√£o **removidos** nas camadas anal√≠ticas.  
- **Leitura do A/B (CSV dentro de tar.gz)**: ignoramos arquivos ‚Äúfantasma‚Äù do macOS (`._*.csv`, `.DS_Store`) e escolhemos o **maior CSV v√°lido**.  
- **Outliers**: estat√≠sticas de cauda s√£o inspecionadas no ETL; a decis√£o de **winsorizar** (ex.: 1‚Äì99%) ser√° aplicada no notebook do A/B.  
- **Reprodutibilidade**: o **download √© program√°tico** e o pipeline √© execut√°vel **no Colab** sem setup manual.

---

## üß™ Checks r√°pidos no ETL (no final do `01_etl_pyspark`)

- Unicidade: `order_id` e `customer_id` distintos nas silvers.  
- Nulos por coluna (especial aten√ß√£o a `event_ts_utc`, `order_total_amount`, `is_target`).  
- Faixa de datas (UTC) e contagem di√°ria (BRT) por grupo (`is_target`).  
- Split do A/B (equil√≠brio entre controle e tratamento).  
- Resumo/quantis de `order_total_amount` e inspe√ß√£o de outliers.

---

## üó∫Ô∏è Roadmap (pr√≥ximas entregas)

- `02_abtest_core.ipynb`: m√©tricas por usu√°rio (GMV/U, Pedidos/U, Convers√£o, AOV), **Welch t-test** e **z-test de propor√ß√µes**; (opcional) **CUPED**.
- `03_financial_viability.ipynb`: **ROI** e an√°lise de **sensibilidade** (take rate, custo do cupom, cobertura).
- `04_segments_ab_by_segment.ipynb`: **RFM** + leitura do **uplift por segmento** e recomenda√ß√µes direcionadas.

---

## üìÑ Licen√ßa & Privacidade

- Dados **n√£o s√£o versionados**. O script `scripts/download_data.py` baixa os insumos a partir das URLs configuradas.  
- PII √© protegida nas **camadas anal√≠ticas** (hash/removidos).  
- Reposit√≥rio preparado para execu√ß√£o em **ambiente gerenciado** (Colab) e **local** (com as observa√ß√µes de Windows acima).
