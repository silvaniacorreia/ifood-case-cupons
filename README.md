# Case iFood: Teste A/B Estrat√©gia de Cupons

Reposit√≥rio do case para **Analista de Dados** no iFood. Objetivo: analisar um **teste A/B** de cupons com foco em **reten√ß√£o** e crescimento.

> Execu√ß√£o **100% no Google Colab** para m√°xima reprodutibilidade (sem depend√™ncias locais).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

---

## üß≠ Escopo da an√°lise

1) **A/B de cupons**  
   (a) Definir **m√©tricas de sucesso** e checar **signific√¢ncia** no per√≠odo;  
   (b) **Viabilidade financeira** (ROI/payback) com **premissas expl√≠citas**;  
   (c) Recomendar **melhorias** e desenhar um **novo A/B** (m√©tricas/guardrails).

2) **Segmenta√ß√£o de usu√°rios**  
   (a) **RFM** como baseline (crit√©rios e racional claros);  
   (b) Ler o **efeito do A/B por segmento** e propor **a√ß√µes por p√∫blico**.

3) **Pr√≥ximos passos**  
   Estimativa de **impacto** (financeiro ou n√£o) e sugest√µes de **processo/teste**.

---

## üóÇÔ∏è Estrutura do reposit√≥rio

```
ifood-case-cupons/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ pipeline_analise_completa.ipynb  # notebook principal (orquestra√ß√£o da an√°lise)
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ utils.py                       # settings + spark + seeds
‚îÇ  ‚îî‚îÄ etl.py                         # ingest√£o + limpeza + joins + silvers
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
‚îú‚îÄ config/
‚îÇ  ‚îî‚îÄ settings.yaml                  # fontes + par√¢metros (ver abaixo)
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                           # arquivos baixados
‚îÇ  ‚îî‚îÄ processed/                     # parquet gerados pelo ETL
‚îî‚îÄ report/                           # relat√≥rio final (PDF)
```

---

## üß± Arquitetura & otimiza√ß√µes atuais

### ETL otimizado
1. **Janela de an√°lise antes do join (robusta a outliers)**  
   A janela √© **inferida automaticamente pelos quantis 1‚Äì99% de `event_ts_utc`** e aplicada **antes** dos joins.  
   **Impacto:** reduz shuffle, mem√≥ria e custo de join sem ser ‚Äúpuxada‚Äù por timestamps an√¥malos.

2. **Proje√ß√£o m√≠nima de colunas**  
   Selecionamos apenas as colunas necess√°rias antes dos joins.  
   **Impacto:** menos shuffle/mem√≥ria, joins e writes mais r√°pidos.

3. **Reparticionamento por chave de join**  
   `orders` √© reparticionado por `customer_id` usando `spark.sql.shuffle.partitions`.  
   **Impacto:** melhor balanceamento no shuffle durante o join.

4. **Broadcast de dimens√£o pequena**  
   `restaurants` √© broadcast para habilitar broadcast-hash join.  
   **Impacto:** elimina shuffle dessa dimens√£o e acelera o join.

5. **Controles de verbosidade e cache**  
   `verbose=False` por padr√£o (evita `count()`/`show()` desnecess√°rios) e `cache_intermediates=False` (reduz risco de OOM).

6. **Escrita em Parquet opcional e leve**  
   Persist√™ncia √© **opcional** (desligada por padr√£o). Ao salvar, usamos `partitionBy(event_date_brt)` e **coalesce** para evitar explos√£o de arquivos.

### Configura√ß√µes do Spark para Colab
- **AQE ligado:** `spark.sql.adaptive.enabled=true`
- **Serializer Kryo:** menor overhead de serializa√ß√£o
- **Timezone fixa (UTC):** convers√µes consistentes
- **`spark.sql.shuffle.partitions`:** 32 (definido por testes de benchmark) via `config/settings.yaml`

**Exemplo de `runtime.spark.conf` no Colab:**
```yaml
runtime:
  spark:
    app_name: "ifood-case-cupons"
    driver_memory: "12g"
    shuffle_partitions: 128
    conf:
      spark.master: "local[*]"
      spark.sql.adaptive.enabled: "true"
      spark.sql.adaptive.coalescePartitions.enabled: "true"
      spark.sql.files.maxPartitionBytes: "64m"
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"
      spark.memory.fraction: "0.6"
      spark.sql.autoBroadcastJoinThreshold: "50MB"
```

### Leituras e conformiza√ß√µes robustas
- **Reader resiliente de orders:** detecta NDJSON ou JSON array e tem fallback com gzip/json.
- **Sanitiza√ß√£o de PII e tipagem:** normalizamos timestamps, lat/long, flags, calculamos `basket_size` de forma segura, e removemos/hasheamos PII.
- **Checagens pr√©vias (preflight):** verificamos tamanhos, compress√£o e candidatos do `ab_test_ref` antes de acionar o Spark; falha cedo em caso de problema.

### Enriquecimentos para an√°lise (camada ‚Äúsilver‚Äù)
- **`origin_platform` nulo ‚Üí `"unknown"`** (evita perdas em cortes por canal).
- **Campos de consumidor faltantes**: vers√µes limpas para segmenta√ß√£o (ex.: `language_clean = coalesce(language, 'unknown')`), mantendo os originais para auditoria.
- **Atributos de restaurante imputados (colunas paralelas)**  
  - `minimum_order_value_imputed`: mediana por (`merchant_city`, `price_range`) com fallback por `price_range`.  
  - `delivery_time_imputed`: mediana por `price_range`.  
  As colunas **originais s√£o preservadas**; as vers√µes imputadas s√£o usadas apenas para diagn√≥stico/controle (ex.: balance check/CUPED).

---

## ‚ñ∂Ô∏è Como executar

### Execu√ß√£o no Colab

1. Abra o notebook **no Colab**:  
   [**pipeline_analise_completa.ipynb**](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/pipeline_analise_completa.ipynb)

2. **Runtime ‚Üí Run all**. A primeira c√©lula:
   - clona/atualiza o reposit√≥rio;
   - instala as depend√™ncias de `requirements.txt`;
   - roda o **download program√°tico** (`scripts/download_data.py`);

3. O notebook ent√£o executa:
   - **Pr√©-flight** (fail-fast) dos arquivos baixados;  
   - **Profiling** dos 4 dataframes brutos;  
   - **ETL** completo com normaliza√ß√£o de timezone/PII e joins;  
   - **A/B**, **viabilidade** e **RFM** (em sequ√™ncia).

> **Tempo de execu√ß√£o:** a leitura de `orders` (~1.6 GB gz) pode levar alguns minutos no Colab (gzip n√£o √© splittable). Depois da leitura, o ETL **reparticiona por `customer_id`** para paralelizar os joins.

---

## ‚öôÔ∏è Configura√ß√µes importantes (`config/settings.yaml`)

| Caminho                         | Descri√ß√£o |
|--------------------------------|-----------|
| `data.raw_dir`                 | Pasta dos brutos (default: `data/raw`) |
| `data.processed_dir`           | Pasta dos parquet (se habilitar salvar) |
| `runtime.spark.driver_memory`  | Mem√≥ria do driver no Colab (`12g`) |
| `analysis.business_tz`         | TZ de neg√≥cio (default `America/Sao_Paulo`) |
| `analysis.experiment_window`   | **auto-inferida** |
| `analysis.auto_infer_window`   | `true`/`false` ‚Äî ativa a infer√™ncia de janela |
| `analysis.treat_is_target_null_as_control` | `false` por padr√£o (linhas sem grupo s√£o exclu√≠das) |
| `analysis.winsorize`/`use_cuped` | Par√¢metros para A/B (aplicados nas an√°lises) |
| `runtime.spark.conf.*`        | Confs avan√ßadas do Spark (AQE, Kryo, parti√ß√µes, broadcast etc.) |

**Exemplo de `runtime.spark.conf` usado no Colab:**
```yaml
runtime:
  spark:
    app_name: "ifood-case-cupons"
    driver_memory: "12g"
    shuffle_partitions: 32
    conf:
      spark.master: "local[*]"
      spark.sql.adaptive.enabled: "true"
      spark.sql.adaptive.coalescePartitions.enabled: "true"
      spark.sql.files.maxPartitionBytes: "64m"
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"
      spark.memory.fraction: "0.6"
      spark.sql.autoBroadcastJoinThreshold: "50MB"
```

---

## üß± Decis√µes t√©cnicas & otimiza√ß√µes de desempenho

### Formato e leitura dos dados
- **Formato de `orders`**: detectado como **NDJSON**; leitura com `spark.read.json(...)`.  
  - Como o arquivo √© grande e gzip n√£o √© splittable, a leitura inicial roda em 1 task; ap√≥s ler, fazemos:  
    **`o = o.repartition(spark.sql.shuffle.partitions, 'customer_id')`** para **distribuir** o trabalho nos joins.
  - **Broadcast** de dimens√µes: `restaurants` sempre (pequena) e `abmap` se `count ‚â§ 2M`.

### Configura√ß√£o do Spark
- **driver_memory**: ajustado para **12g** para evitar problemas de mem√≥ria.

### Persist√™ncia e formato de sa√≠da
- **Persist√™ncia estrat√©gica**: usamos `.cache()` para evitar recomputa√ß√£o em etapas subsequentes.  
- **Parquet**: ap√≥s o ETL, os DataFrames processados podem ser salvos em Parquet para acelerar leituras futuras.

---

## üß∞ O notebook como orquestrador t√©cnico

O notebook **pipeline_analise_completa.ipynb** funciona como um **orquestrador t√©cnico** das tarefas de an√°lise, integrando os diferentes m√≥dulos do reposit√≥rio:

1. **Configura√ß√£o inicial**:
   - Clona o reposit√≥rio e instala as depend√™ncias.
   - Faz o download program√°tico dos dados brutos.

2. **Configura√ß√£o do Spark**:
   - L√™ as configura√ß√µes do arquivo `settings.yaml` e inicializa o Spark com par√¢metros ajustados para o Colab.

3. **Execu√ß√£o das tarefas**:
   - **Pr√©-flight**: valida√ß√£o dos arquivos brutos.
   - **ETL**: utiliza fun√ß√µes do m√≥dulo `src/etl.py` para ingest√£o, conforma√ß√£o e gera√ß√£o dos DataFrames "silver".
   - **An√°lises**: executa m√©tricas de A/B, ROI e segmenta√ß√£o, utilizando fun√ß√µes espec√≠ficas dos m√≥dulos `src/utils.py` e `src/checks.py`.

4. **Orquestra√ß√£o**:
   - O notebook organiza a execu√ß√£o das etapas de forma sequencial, garantindo que cada tarefa seja realizada com base nos resultados da anterior.

---

## ‚úÖ Pr√©-flight & Profiling (o que verificar ao apresentar)

- **Pr√©-flight (fail-fast)**: arquivos existem, tamanhos coerentes, gzip/tar √≠ntegros, **CSVs v√°lidos** do A/B encontrados.  
- **Profiling (p√≥s-leitura)**:  
  - `orders/consumers/restaurants/abmap`: **schema** e **amostras**;  
  - faixa de datas (min/max) e **nulos em campos-chave**;  
  - distribui√ß√£o de **grupo** (controle vs tratamento) no A/B.

Esses passos mostram maturidade de engenharia e evitam ‚Äúrodar com tabelas vazias‚Äù.

---

## üì¶ Sa√≠das do ETL (em mem√≥ria)

- `orders_silver`: fato por pedido (UTC/BRT, valores, flags, atributos do consumidor e do restaurante, `is_target`).  
- `users_silver`: R/F/M por usu√°rio + `is_target`, com `recency` calculado a partir do √∫ltimo `event_ts_utc`.

---

## üìà A/B, ROI e Segmenta√ß√£o (no notebook)

- **A/B**: m√©tricas por usu√°rio (**GMV/U, Pedidos/U, Convers√£o, AOV**), **Welch t-test** (m√©dias) e **z-test** (propor√ß√µes). Opcional: **CUPED**.  
- **Viabilidade**: **ROI** e **sensibilidade** (take rate, custo do cupom, cobertura). **Premissas** ficam expl√≠citas no topo da se√ß√£o.  
- **Segmenta√ß√£o (RFM)**: regras claras e leitura do **uplift por segmento**, com **a√ß√µes sugeridas** por p√∫blico.
- **Melhorias futuras**: **K-Means** como refinamento da segmenta√ß√£o; **guardrails** adicionais para o novo A/B.

---

## üîí Privacidade

- Dados PII **n√£o** s√£o mantidos nas camadas anal√≠ticas (hash/removidos).  
- Os arquivos de dados **n√£o** s√£o versionados no Git; sempre baixados de fontes configuradas em `settings.yaml`.

---

## üìå Resumo para a apresenta√ß√£o

- **Por que Colab-only?** Reprodutibilidade e simplicidade para os avaliadores.  
- **Gargalo conhecido:** `orders` √© grande e gzip n√£o √© splittable ‚Üí leitura 1 task; depois **repartition + broadcast**.  
- **Qualidade:** pr√©-flight fail-fast + profiling guiando o ETL; timezone/PII/valida√ß√µes.  
- **A/B ‚Üí ROI ‚Üí RFM** na ordem pedida, com **premissas expl√≠citas** e **pr√≥ximos passos**.

---
