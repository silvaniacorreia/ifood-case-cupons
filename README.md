# iFood Case ‚Äì Cupons

Reposit√≥rio do case para **Analista de Dados** no iFood. Objetivo: analisar um **teste A/B** de cupons com foco em **reten√ß√£o** e crescimento, seguindo **exatamente** a ordem pedida no case (A/B ‚Üí viabilidade financeira ‚Üí segmenta√ß√£o ‚Üí recomenda√ß√µes).

> Execu√ß√£o **100% no Google Colab** para m√°xima reprodutibilidade (sem depend√™ncias locais).

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/analise_completa.ipynb){:target="_blank" rel="noopener"}

---

## üîÑ O que mudou desde a √∫ltima vers√£o

- **Colab-only**: removida a necessidade de execu√ß√£o local (JDK/winutils etc.). O notebook faz **clone do repo**, **instala depend√™ncias** e **baixa dados** sozinho.
- Novo **pr√©-flight fail-fast** (`src/checks.py`): valida exist√™ncia/tamanho/integra√ß√£o (`.gz` e `.tar.gz`) e identifica **CSVs v√°lidos** do A/B (ignora artefatos `._*.csv`, `.DS_Store`).
- **Leitura robusta** de `orders` em `src/etl.py`:
  - Detec√ß√£o autom√°tica **NDJSON** (um JSON por linha) **vs** **JSON array**.
  - Para NDJSON (nosso caso), usamos `spark.read.json(...)`.  
- **Orders √© muito grande** (~**1.6 GB** comprimido): gzip **n√£o √© splittable**, ent√£o a descompress√£o ocorre em **um √∫nico task**; ap√≥s ler, **reparticionamos** o dataset conformado por `customer_id` (usando `spark.sql.shuffle.partitions`) para **paralelizar os joins**.  
  - Tamb√©m **broadcast** das dimens√µes pequenas: `restaurants` (sempre) e `abmap` (se couber).
- **Profiling r√°pido** p√≥s-leitura (`profile_loaded`): mostra **schema**, **amostras**, **faixa de datas**, **nulos** e **distribui√ß√£o** do grupo do A/B ‚Äî orienta as regras do ETL.
- **Janela do experimento**: se n√£o vier no `settings.yaml`, √© **inferida automaticamente** como `[min(data), max(data)+1d)` em UTC e aplicada como filtro.
- **PII**: `cpf`/telefone **hasheados**; campos de endere√ßo/nome **removidos** das camadas anal√≠ticas.
- **Camadas ‚Äúsilver‚Äù em mem√≥ria**: salvar Parquet √© **opcional** (desativado por padr√£o no Colab para reduzir I/O).

---

## üß≠ Escopo da an√°lise

1) **A/B de cupons**  
   (a) Definir **m√©tricas de sucesso** e checar **signific√¢ncia** no per√≠odo;  
   (b) **Viabilidade financeira** (ROI/payback) com **premissas expl√≠citas**;  
   (c) Recomendar **melhorias** e desenhar um **novo A/B** (m√©tricas/guardrails).

2) **Segmenta√ß√£o de usu√°rios**  
   (a) **RFM** como baseline (crit√©rios e racional claros);  
   (b) Ler o **efeito do A/B por segmento** e propor **a√ß√µes por p√∫blico**.

3) **Pr√≥ximos passos**  
   Estimativa de **impacto** (financeiro ou n√£o) e sugest√µes de **processo/teste**.
---

## üóÇÔ∏è Estrutura do reposit√≥rio

```
ifood-case-cupons/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ notebooks/
‚îÇ  ‚îú‚îÄ 00_setup_and_checks.ipynb      # bootstrap + download program√°tico + smoke Spark
‚îÇ  ‚îú‚îÄ 01_etl_pyspark.ipynb           # ETL e silvers (orders/users)
‚îÇ  ‚îú‚îÄ 02_abtest_core.ipynb           # (em constru√ß√£o) A/B m√©tricas + signific√¢ncia
‚îÇ  ‚îú‚îÄ 03_financial_viability.ipynb   # (em constru√ß√£o) ROI e sensibilidade
‚îÇ  ‚îî‚îÄ 04_segments_ab_by_segment.ipynb# (em constru√ß√£o) RFM + leitura do A/B por segmento
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ utils.py                       # settings + spark + seeds
‚îÇ  ‚îî‚îÄ etl.py                         # ingest√£o + limpeza + joins + silvers
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ download_data.py               # baixa .gz/.tar.gz; extrai tar e limpa artefatos
‚îú‚îÄ config/
‚îÇ  ‚îú‚îÄ settings.example.yaml
‚îÇ  ‚îî‚îÄ settings.yaml                  # fontes + par√¢metros (ver abaixo)
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ raw/                           # arquivos baixados
‚îÇ  ‚îî‚îÄ processed/                     # parquet gerados pelo ETL
‚îî‚îÄ report/                           # relat√≥rio final (PDF)
```

---

## ‚ñ∂Ô∏è Como executar

1. Abra o notebook **no Colab**:  
   [**analise_completa.ipynb**](https://colab.research.google.com/github/silvaniacorreia/ifood-case-cupons/blob/main/notebooks/analise_completa.ipynb){:target="_blank" rel="noopener"}

2. Menu **Runtime ‚Üí Run all**. A primeira c√©lula:
   - clona/atualiza o reposit√≥rio;  
   - instala as depend√™ncias do `requirements.txt`;  
   - roda o **download program√°tico** (`scripts/download_data.py`).

3. O notebook ent√£o executa:
   - **Pr√©-flight** (fail-fast) dos arquivos baixados;  
   - **Profiling** dos 4 dataframes brutos;  
   - **ETL** completo com normaliza√ß√£o de timezone/PII e joins;  
   - **A/B**, **viabilidade** e **RFM** (em sequ√™ncia).

> **Tempo de execu√ß√£o:** a leitura de `orders` (~1.6 GB gz) pode levar alguns minutos no Colab (gzip n√£o √© splittable). Depois da leitura, o ETL **reparticiona por `customer_id`** para paralelizar os joins.

---

## ‚öôÔ∏è Par√¢metros importantes (`config/settings.yaml`)

| Caminho                         | Descri√ß√£o |
|--------------------------------|-----------|
| `data.raw_dir`                 | Pasta dos brutos (default: `data/raw`) |
| `data.processed_dir`           | Pasta dos parquet (se habilitar salvar) |
| `runtime.spark.shuffle_partitions` | N¬∫ de parti√ß√µes p/ shuffles/joins (usado no `repartition`) |
| `runtime.spark.driver_memory`  | Mem√≥ria do driver no Colab (ex.: `8g`/`12g`) |
| `analysis.business_tz`         | TZ de neg√≥cio (default `America/Sao_Paulo`) |
| `analysis.experiment_window`   | `{start: 'YYYY-MM-DD', end: 'YYYY-MM-DD'}`; se ausente, **auto-inferida** |
| `analysis.auto_infer_window`   | `true`/`false` ‚Äî ativa a infer√™ncia de janela |
| `analysis.treat_is_target_null_as_control` | `false` por padr√£o (linhas sem grupo s√£o exclu√≠das) |
| `analysis.winsorize`/`use_cuped` | Par√¢metros para A/B (aplicados nas an√°lises) |

---

## üß± Decis√µes t√©cnicas & premissas (essenciais para explicar)

- **Formato de `orders`**: detectado como **NDJSON**; leitura com `spark.read.json(...)`.  
  - Como o arquivo √© grande e gzip n√£o √© splittable, a leitura inicial roda em 1 task; ap√≥s ler, fazemos:  
    **`o = o.repartition(spark.sql.shuffle.partitions, 'customer_id')`** para **distribuir** o trabalho nos joins.
  - **Broadcast** de dimens√µes: `restaurants` sempre (pequena) e `abmap` se `count ‚â§ 2M`.
- **Timezone**: criamos `event_ts_utc` (UTC can√¥nico). Para an√°lises di√°rias de neg√≥cio, usamos `event_date_brt` (UTC ‚Üí BRT).  
- **Evento**: se `order_scheduled==true` e h√° `order_scheduled_date`, o **evento** √© a data agendada; caso contr√°rio, a de cria√ß√£o.  
- **PII**: `cpf` e telefone geram `*_hash`; campos sens√≠veis (nome/endere√ßo) **removidos** nas ‚Äúsilver‚Äù.  
- **Qualidade de dados**:  
  - descartamos `order_id` ou `customer_id` **nulos**, `order_id` **duplicado**;  
  - `order_total_amount` negativo vira **nulo**;  
  - coordenadas de `merchant_*` fora do intervalo v√°lido viram **nulo**.  
- **Janela do experimento**: se n√£o definida, inferimos `[min, max+1)` e reportamos no log (UTC).  
- **Sem marca√ß√£o A/B (`is_target` nulo)**: **exclu√≠mos** por padr√£o (evitar vi√©s); pode ser mudado por config.  
- **Persist√™ncia**: por padr√£o **n√£o** salvamos Parquet no Colab (`SAVE_PARQUET=False`) ‚Äî tudo roda **em mem√≥ria**; habilitar apenas se necess√°rio.

---

## ‚úÖ Pr√©-flight & Profiling (o que verificar ao apresentar)

- **Pr√©-flight (fail-fast)**: arquivos existem, tamanhos coerentes, gzip/tar √≠ntegros, **CSVs v√°lidos** do A/B encontrados.  
- **Profiling (p√≥s-leitura)**:  
  - `orders/consumers/restaurants/abmap`: **schema** e **amostras**;  
  - faixa de datas (min/max) e **nulos em campos-chave**;  
  - distribui√ß√£o de **grupo** (controle vs tratamento) no A/B.

Esses passos mostram maturidade de engenharia e evitam ‚Äúrodar com tabelas vazias‚Äù.

---

## üì¶ Sa√≠das do ETL (em mem√≥ria)

- `orders_silver`: fato por pedido (UTC/BRT, valores, flags, atributos do consumidor e do restaurante, `is_target`).  
- `users_silver`: R/F/M por usu√°rio + `is_target`, com `recency` calculado a partir do √∫ltimo `event_ts_utc`.

---

## üìà A/B, ROI e Segmenta√ß√£o (no notebook)

- **A/B**: m√©tricas por usu√°rio (**GMV/U, Pedidos/U, Convers√£o, AOV**), **Welch t-test** (m√©dias) e **z-test** (propor√ß√µes). Opcional: **CUPED**.  
- **Viabilidade**: **ROI** e **sensibilidade** (take rate, custo do cupom, cobertura). **Premissas** ficam expl√≠citas no topo da se√ß√£o.  
- **Segmenta√ß√£o (RFM)**: regras claras e leitura do **uplift por segmento**, com **a√ß√µes sugeridas** por p√∫blico.
- **Melhorias futuras**: **K-Means** como refinamento da segmenta√ß√£o; **guardrails** adicionais para o novo A/B.

---

## üß∞ Solu√ß√£o de problemas (Colab)

- **Execu√ß√£o lenta no come√ßo**: esperado por causa de `orders` (~1.6 GB gz, gzip n√£o splittable). Ap√≥s a leitura, o ETL paraleliza.  
- **Out of memory**: aumente `runtime.spark.driver_memory` no `settings.yaml` (ex.: `12g`) e reduza o n√∫mero de colunas exibidas (menos `.toPandas()` em previews).  
- **Erro de rede no download**: reexecute a 1¬™ c√©lula (o script √© idempotente).  
- **CSV do A/B n√£o encontrado**: o pr√©-flight falha cedo e indica o problema na pasta `data/raw/ab_test_ref_extracted/`.

---

## üîí Privacidade

- Dados PII **n√£o** s√£o mantidos nas camadas anal√≠ticas (hash/removidos).  
- Os arquivos de dados **n√£o** s√£o versionados no Git; sempre baixados de fontes configuradas em `settings.yaml`.

---

## üìå Resumo para a apresenta√ß√£o

- **Por que Colab-only?** Reprodutibilidade e simplicidade para os avaliadores.  
- **Gargalo conhecido**: `orders` √© grande e gzip n√£o splittable ‚Üí leitura 1 task; depois **repartition + broadcast**.  
- **Qualidade**: pr√©-flight fail-fast + profiling guiando o ETL; timezone/PII/valida√ß√µes.  
- **A/B ‚Üí ROI ‚Üí RFM** na ordem pedida, com **premissas expl√≠citas** e **pr√≥ximos passos**.

---
